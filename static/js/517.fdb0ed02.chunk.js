"use strict";(self.webpackChunkmy_app=self.webpackChunkmy_app||[]).push([[517],{5224:function(e,a,t){t.d(a,{z:function(){return r}});var i,n=t(168),o=(0,t(1191).ZP)("button")(i||(i=(0,n.Z)(["\n  background: ",";\n  color: ",";\n  font-size: 1rem;\n  font-weight: 700;\n  width: 100%;\n  border: 1px solid #edf3f5;\n  border-radius: 4px;\n  padding: 13px 0;\n  cursor: pointer;\n  margin-top: 0.625rem;\n  max-width: 180px;\n  transition: all 0.3s ease-in-out;\n  box-shadow: 0 16px 30px rgb(23 31 114 / 20%);\n\n  &:hover,\n  &:active,\n  &:focus {\n    color: #fff;\n    border: 1px solid rgb(255, 130, 92);\n    background-color: rgb(255, 130, 92);\n  }\n"])),(function(e){return e.color||"#2e186a"}),(function(e){return e.color?"#2E186A":"#fff"})),s=t(184),r=function(e){var a=e.color,t=e.children,i=e.onClick;return(0,s.jsx)(o,{color:a,onClick:i,children:t})}},4260:function(e){e.exports=JSON.parse('[{"type":"seminar","title":"Mechanistic Understanding of Language Models in Arithmetic Reasoning and Code Generation","speaker":"Ziyu Yao","affiliation":"George Mason University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 05 December 2024","time":" 14:00 - 15:00","abstract":"Transformer-based language models (LMs) have demonstrated the promise in solving more and more complicated tasks, yet alongside their advancements are growing concerns on their safety and reliability. These concerns primarily stem from our limited understanding of these LMs and the difficulty in interpreting their behaviors. In this talk, I will present our two recent projects towards forming a mechanistic understanding of LMs. In the first project (published at  ACL \u201924), we explain how Chain-of-Thoughts (CoT) elicit the arithmetic reasoning of LMs by looking into the neuron activation inside the models; in the second project (ongoing), we generalize the analysis to understand the mechanism of how LMs solve the structured code generation problem. Finally, I will conclude the talk by briefly sharing our other effort along the line of LM planning and interpretability.","bio":"Ziyu Yao (https://ziyuyao.org/) is an Assistant Professor in the Department of Computer Science at George Mason University, where she co-leads the George Mason  NLP  group (https://nlp.cs.gmu.edu/). Her research topics cover LLMs, semantic parsing/code generation, model interpretability, and human-AI interaction. Her work has been funded by National Science Foundation, Virginia Commonwealth Cyber Initiative, Microsoft\u2019s Accelerating Foundation Models Research Program, among others. She has regularly served as an area chair at top-tier  NLP /AI conferences and was the Diversity & Inclusion Co-Chair at  NAACL 2024 . Prior to George Mason, she graduated with a Ph.D. degree in Computer Science and Engineering from the Ohio State University in 2021, where she was awarded the prestigious Presidential Fellowship. She was also selected as a rising star in  EECS  by  UC  Berkeley in 2021."},{"type":"seminar","title":"Data Repurposing: Improving LLM Capabilities with Synthetic Data Generation","speaker":"Abdullatif K\xf6ksal","affiliation":"LMU Munich","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"hybrid","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 28 November 2024","time":" 11:00 - 12:00","abstract":"Creating high-quality, diverse, large-scale datasets remains a critical and time-consuming challenge in improving  LLM  capabilities. Motivated by prior work that manually identifies implicit signals in raw corpora, we aim to address this challenge by investigating data repurposing strategies, a methodology for automatically transforming existing data resources into new formats and purposes. First, we propose reverse instructions to build an English instruction-following dataset by synthetically generating instructions for a given human-written corpus document. The model trained with our synthetic dataset performs significantly better than other instruction-following models, especially in long-form generation. Next, in  MURI , we extend this approach to 200 languages to create a culturally inclusive, native dataset and multilingual instruction-following models for very low-resource languages. Finally, we customize this approach to generate any downstream dataset in  CRAFT , targeting unannotated corpora to synthesize custom downstream task examples by retrieving and rewriting corpus documents using few-shot examples. Our experiments demonstrate that this approach can generate large-scale datasets for any given task, showing up to a 25% improvement in tasks such as biology QA and summarization compared to few-shot settings.","bio":"Abdullatif K\xf6ksal is a final-year  ELLIS  PhD student at  CIS , LMU Munich and  LTL , University of Cambridge, supervised by Prof. Hinrich Sch\xfctze and Prof. Anna Korhonen. His research focuses on improving  LLM  capabilities through effective data utilization and synthetic data generation. He has proposed several works around data repurposing by restructuring and augmenting existing data resources, including reverse instructions for long-form instruction-tuning and a culturally-respectful multilingual instruction-following dataset for 200 languages. He expanded these approaches to dataset generation for downstream tasks through better corpus mining with LLMs in  CRAFT . He worked in other areas such as counterfactuality, robustness, and multilinguality and published multiple papers in top-tier  NLP  venues. He interned at Google and Amazon, where he worked on counterfactuality and faithfulness."},{"type":"seminar","title":"LLMs and Low-Resource Languages","speaker":"Eneko Agirre","affiliation":"University of the Basque Country (UPV/EHU)","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"hybrid","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Wednesday 27 November 2024","time":" 16:00 - 17:00","abstract":"Generative AI models are now multilingual, raising new questions about their relative performance across languages and local cultures, specially for communities with less speakers. In this talk I will explore some of those questions and the lessons we learned along the process. Is it possible to build high-performing LLMs for low-resource languages? We have built a high performing open model for Basque accompanied by a fully reproducible end-to-end evaluation suite. Do LLMs think better in English than the local language? Our experiments show that LLMs do not fully exploit their multilingual potential when prompted in non-English languages. Do LLMs know about local culture? We probed the complex interaction between language and global/local knowledge, showing for the first time that local knowledge is transferred from the low-resource to the high-resource language, a sign that prior findings may not hold when evaluated on local topics. The evaluation suite was recognised with a best resource paper award at  ACL 2024 .","bio":"Eneko Agirre is Full Professor of Informatics and Head of HiTZ Basque Center of Language Technology at the University of the Basque Country,  UPV /EHU, in San Sebastian, Spain.\\nVisiting researcher or professor at New Mexico State, Melbourne, Southern California, Stanford and New York Universities. He has been active in Natural Language Processing and Computational Linguistics since his undergraduate days. He received the Spanish Informatics Research Award in 2021, and is one of the 74 fellows of the Association of Computational Linguistics (ACL). He was President of  ACL \u2019s  SIGLEX , member of the editorial board of Computational Linguistics, Journal of Artificial Intelligence Research and Action Editor for the Transactions of the  ACL . He is co-founder of the Joint Conference on Lexical and Computational Semantics (*SEM). He is a recipient of three Google Research Awards and six best paper awards and nominations, most recent at  ACL 2024 . Dissertations under his supervision received best PhD awards by EurAI, the Spanish  NLP  society and the Spanish Informatics Scientific Association. He has over 200 publications across a wide range of  NLP  and AI topics, as well as having given more than 20 invited talks, mostly international."},{"type":"seminar","title":"Towards Knowledgeable Foundation Models","speaker":"Heng Ji","affiliation":"UIUC","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 21 November 2024","time":" 15:30 - 16:30","abstract":"Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from extensive pretraining data. However, their inherent knowledge bases often suffer from disorganization and illusion, bias towards common entities, and rapid obsolescence. Consequently, LLMs frequently make up untruthful information, exhibit resistance to updating outdated knowledge, or struggle with generalizing across multiple languages. In this talk I will discuss several research directions that aim to make foundation models\u2019 knowledge more accurate, organized, up-to-date and fair: (1) Where and How is Knowledge Stored in  LLM ? (2) How to Control  LLM \u2019s Knowledge? (3) How to Update  LLM \u2019s Dynamic Knowledge? (4) How to Bridge the Knowledge Gap between Natural Language and Unnatural Language?","bio":"Heng Ji is a professor at Siebel School of Computing and Data Science, and an affiliated faculty member at Electrical and Computer Engineering Department, Coordinated Science Laboratory, and Carl R. Woese Institute for Genomic Biology of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE). She received her B.A. and M. A. in Computational Linguistics from Tsinghua University, and her M.S. and Ph.D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models and Vision-Language Models, and AI for Science. The awards she received include Outstanding Paper Award at  ACL2024 , two Outstanding Paper Awards at  NAACL2024 , \u201cYoung Scientist\u201d by the World Laureates Association in 2023 and 2024, \u201cYoung Scientist\u201d and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017, \u201cWomen Leaders of Conversational AI\u201d (Class of 2023) by Project Voice, \u201cAI\u2019s 10 to Watch\u201d Award by  IEEE  Intelligent Systems in 2013,  NSF CAREER  award in 2009,  PACLIC2012  Best paper runner-up, \u201cBest of  ICDM2013 \u201d paper award, \u201cBest of  SDM2013 \u201d paper award,  ACL2018  Best Demo paper nomination,  ACL2020  Best Demo Paper Award,  NAACL2021  Best Demo Paper Award, Google Research Award in 2009 and 2014,  IBM  Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She served as the associate editor for  IEEE /ACM Transaction on Audio, Speech, and Language Processing, and the Program Committee Co-Chair of many conferences including  NAACL -HLT2018 and  AACL -IJCNLP2022. She was elected as the North American Chapter of the Association for Computational Linguistics (NAACL) secretary 2020-2023."},{"type":"seminar","title":"Subtleties about Pre-Training Data: Imbalance and Staleness","speaker":"Daniel Khashabi","affiliation":"Johns Hopkins University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 07 November 2024","time":" 16:00 - 17:00","abstract":"The success of pre-trained large language models (LLMs) is largely attributed to the extensive and diverse data used during their pre-training phase. Leveraging this pre-training effectively can lead to notable improvements in model quality, robustness, and cost-efficiency. Firstly, I will address the challenges of [pre-]training on imbalanced datasets, such as those found in multilingual settings where data availability varies greatly between high- and low-resource languages. Common approaches to mitigate this issue include upsampling low-resource languages or enhancing their loss weight. Although these methods are often seen as equivalent, I will demonstrate through theoretical and empirical evidence that they are distinct. Based on these insights, we propose a strategy for efficient and balanced training on imbalanced datasets. Secondly, I will investigate the issue of temporal degradation in LLMs, which arises after the cutoff dates for training data collection. Our empirical evidence indicates that this degradation often begins well before the stated cutoff, a point we call the \u201ceffective cutoff\u201d date. I will discuss our analysis of open pre-training datasets, which uncovers the main causes for these observations. These findings imply that knowledge cutoffs are more intricate than previously thought, necessitating careful consideration from both  LLM  dataset curators and users.\\nBased on the following works:\\n1. Upsample or Upweight? Balanced Training on Heavily Imbalanced Datasets: https://arxiv.org/abs/2410.04579\\n2. Dated Data: Tracing Knowledge Cutoffs in Large Language Models: https://arxiv.org/abs/2403.12958","bio":"Daniel Khashabi is an assistant professor of computer science at Johns Hopkins University and is affiliated with the Center for Language and Speech Processing (CLSP) and the Data Science and  AI  Institute. He is interested in building reasoning-driven modular  NLP  systems that are robust, transparent, and communicative, particularly those that use natural language as the communication medium. Khashabi has published over 50 papers on natural language processing and AI in top-tier venues. His research has won the best paper awards at  COLM  (2024),  ACL  (2023), and  NAACL  (2022), Amazon Research Award (2022) and  AI2 \u2019s Last Impact Award (2024). Before joining Hopkins, he was a postdoctoral fellow at the Allen Institute for  AI  (2019-2022) and obtained a Ph.D. from the University of Pennsylvania in 2019."},{"type":"seminar","title":"Pluralistic Alignment through Personalized Reinforcement Learning from Human Feedback","speaker":"Natasha Jaques","affiliation":"University of Washington","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 31 October 2024","time":" 17:00 - 18:00","abstract":"Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for aligning foundation models to human values and preferences. However, current  RLHF  techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population. When these differences arise, traditional  RLHF  frameworks simply average over them, leading to inaccurate rewards and poor performance for minority groups. To address the need for pluralistic alignment, we develop a novel multimodal  RLHF  method, which we term Variational Preference Learning (VPL). In this talk, I will first give an overview of past approaches to  RLHF , and then show how  VPL  address issues of value monism.  VPL  uses a few preference labels to infer a novel user-specific latent variable, and learns reward models and policies conditioned on this latent without additional user-specific data. While conceptually simple, we show that in practice, this reward modeling requires careful algorithmic considerations around model architecture and reward scaling. To empirically validate our proposed technique, we first show that it can provide a way to combat underspecification in simulated control problems, inferring and optimizing user-specific reward functions. Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy. We additionally show the benefits of this probabilistic framework in terms of measuring uncertainty, and actively learning user preferences. This work enables learning from diverse populations of users with divergent preferences, an important challenge that naturally occurs in problems from robot learning to foundation model alignment.","bio":"Natasha Jaques is an Assistant Professor of Computer Science and Engineering at the University of Washington, and a Senior Research Scientist at Google DeepMind. Her research focuses on Social Reinforcement Learning in multi-agent and human-AI interactions. During her PhD at  MIT , she developed techniques for learning from human feedback signals to train language models which were later built on by OpenAI\u2019s series of work on Reinforcement Learning from Human Feedback (RLHF). In the multi-agent space, she has developed techniques for improving coordination through the optimization of social influence, and adversarial environment generation for improving the robustness of RL agents. Natasha\u2019s work has received various awards, including Best Demo at NeurIPS, an honourable mention for Best Paper at  ICML , and the Outstanding PhD Dissertation Award from the Association for the Advancement of Affective Computing. Her work has been featured in Science Magazine,  MIT  Technology Review, Quartz,  IEEE  Spectrum, Boston Magazine, and on  CBC  radio, among others. Natasha earned her Masters degree from the University of British Columbia, and undergraduate degrees in Computer Science and Psychology from the University of Regina."},{"type":"seminar","title":"LLM Generalization in Social Context","speaker":"Diyi Yang","affiliation":"Stanford University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 24 October 2024","time":" 16:00 - 17:00","abstract":"The successes of large language models (LLMs) have transformed many domains, yet they do not always generalize well across all contexts, particularly in areas where social factors are involved. The talk examines  LLM  generalizations in social context from three perspectives: assessment, adaptation, and application. We first present a dynamic evaluation protocol based on directed acyclic graphs with varying complexity for assessing LLMs on many types of reasoning tasks. Then we explore how to adapt LLMs to be more socially generalizable by building culturally aware language technologies with an online-community driven knowledge base. Lastly, we discuss how to customize LLMs for social skill training in a variety of social contexts. Overall, we hope to provide insights into how  LLM  generalizes in social contexts and how to develop socially intelligent LLMs.","bio":"Diyi Yang is an assistant professor in the Computer Science Department at Stanford University. Her research focuses on human-centered natural language processing and computational social science.  She is a recipient of  Microsoft Research Faculty Fellowship (2021),   NSF CAREER  Award (2022), an  ONR  Young Investigator Award (2023), and a Sloan Research Fellowship (2024).   Her work has received multiple paper awards or nominations at top  NLP  and  HCI  conferences."},{"type":"seminar","title":"Specializing LLMs for Factuality and Soft Reasoning","speaker":"Greg Durrett","affiliation":"UT Austin","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 17 October 2024","time":" 14:00 - 15:00","abstract":"Proponents of  LLM  scaling assert that training a giant model on as much data as possible can eventually solve most language tasks, perhaps even leading to  AGI . However, frontier LLMs still fall short on complex problems in long-tail domains. Errors occur somewhere in the process of encoding the necessary knowledge, surfacing it for a specific prompt, and synthesizing it with other input data. In this talk, I will argue that specialization is the right approach to improve LLMs here; that is, modifying them through training or other means to improve their factuality and reasoning capabilities. First, I will show that specialization is necessary: inference-only approaches like chain-of-thought prompting are not sufficient. Second, I will present our fact-checking system MiniCheck, which is fine-tuned on specialized data to detect factual errors in  LLM  responses, leading to a better detector than frontier models like  GPT -4. Finally, I will discuss how to specialize LLMs to be better at logical reasoning. I argue that we need (a) better fine-tuning methods which make targeted adjustments to model behavior; (b) improved inference capabilities, such as a differentiable theorem prover that can be plugged into a standard Transformer. These forms of specialization represent a path towards fundamentally new capabilities in factuality and reasoning beyond what can be achieved in current models.","bio":"Greg Durrett is an associate professor of Computer Science at  UT  Austin. He received his BS in Computer Science and Mathematics from  MIT  and his PhD in Computer Science from  UC  Berkeley, where he was advised by Dan Klein. His research is broadly in the areas of natural language processing and machine learning. Currently, his group\u2019s focus is on techniques for reasoning about knowledge in text, verifying factuality of  LLM  generations, and building systems using LLMs as primitives. He is a 2023 Sloan Research Fellow and a recipient of a 2022  NSF CAREER  award. He has co-organized the Workshop on Natural Language Reasoning and Structured Explanations at  ACL 2023  and  ACL 2024 , as well as workshops on low-resource  NLP  and  NLP  for programming. He has served in numerous roles for *CL conferences, including as a member of the  NAACL  Board since 2024."},{"type":"seminar","title":"Ten Years of Universal Dependencies","speaker":"Joakim Nivre","affiliation":"Uppsala University and RISE","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 10 October 2024","time":" 11:00 - 12:00","abstract":"Universal Dependencies (UD) is a project developing cross-linguistically consistent treebank annotation for many languages, with the goal of facilitating multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective. Since UD was launched almost ten years ago, it has grown into a large community effort involving over 500 researchers around the world, together producing treebanks for 148 languages and enabling new research directions in both  NLP  and linguistics. In this talk, I will review the history and development of UD and discuss challenges that we need to face when bringing UD into the future.","bio":""},{"type":"seminar","title":"Can Sparsity Lead to Efficient LLMs?","speaker":"Shiwei Liu","affiliation":"University of Oxford","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 13 June 2024","time":" 11:00 - 12:00","abstract":"The rapid advancements in Large Language Models (LLMs) have revolutionized various natural language processing tasks. However, the substantial size of LLMs presents significant challenges in training, fine-tuning, and deployment. In this talk, I will discuss how sparsity, a fundamental characteristic in neural networks, can be leveraged to enhance  LLM  efficiency. The presentation will cover recent advances in  LLM  pruning, parameter-efficient fine-tuning, centered on the principle: Not Every Layer in LLMs is Worth Equal Computing.","bio":""},{"type":"seminar","title":"Generative AI for Science","speaker":"James Zou","affiliation":"Stanford University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 06 June 2024","time":" 15:00 - 16:00","abstract":"This talk will explore how we can develop and use generative AI to help researchers and clinicians. I will first discuss how LLMs can act as research co-advisors. Then I will present Dragonfly, our new architecture for large visual-language model that leverages multi-resolution Zoom to achieve state-of-the-art performance across several medical tasks. Finally, I will explore the role of language as the foundational data modality for science.","bio":""},{"type":"seminar","title":"Practical and Specialised NLP Solutions: The Case of Social Media","speaker":"Jose Camacho Collados","affiliation":"Cardiff University","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 30 May 2024","time":" 11:00 - 12:00","abstract":"Large generative language models (LLMs) are currently ubiquitous in  NLP  research. However, when it comes to specific applications, these powerful models are often impractical and not always the optimal choice, and there may be more adequate solutions for specific problems.\\nIn this talk, I will advocate for practical and effective solutions in  NLP . I will try to revisit the importance of high-quality data (including labelled data) and reliable evaluation benchmarks in the current  LLM  landscape. Using social media as a case study, I will explain some of the challenges arising from processing content in these platforms. The multilingual, dynamic, informal and multimodal nature of social media, as well as the constant influx of new content, means that standard techniques are seldom optimal, and specialised models are often required.","bio":""},{"type":"seminar","title":"Incremental Accumulation of Linguistic Context in Artificial and Biological Neural Networks","speaker":"Refael Tikochinski","affiliation":"Hebrew University of Jerusalem","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 23 May 2024","time":" 11:00 - 12:00","abstract":"Accumulated evidence suggests that Large Language Models (LLMs) are beneficial in predicting neural signals related to narrative processing. The way LLMs integrate context over large timescales, however, is fundamentally different from the way the brain does it. In my study, we show that unlike LLMs that apply parallel processing of large contextual windows, the incoming context to the brain is limited to short windows of a few tens of words. We hypothesize that whereas lower-level brain areas process short contextual windows, higher-order areas in the default-mode network (DMN) engage in an online incremental mechanism where the incoming short context is summarized and integrated with information accumulated across long timescales. Consequently, we introduce a novel  LLM  that instead of processing the entire context at once, it incrementally generates a concise summary of previous information. As predicted, we found that neural activities at the  DMN  were better predicted by the incremental model, and conversely, lower-level areas were better predicted with short-context-window  LLM .","bio":""},{"type":"seminar","title":"Data Selection for Pre-training and Instruction-tuning of LLMs","speaker":"Danqi Chen","affiliation":"Princeton University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 16 May 2024","time":" 14:00 - 15:00","abstract":"There is increasing evidence that choosing the right training data is essential for producing state-of-the-art large language models (LLMs). How can we decide on high-quality training data? Can we possibly select fewer data examples to improve performance and efficiency? In this talk, I will present two recent works on selecting high-quality data in pre-training and instruction tuning. I will first present QuRating, a simple framework for selecting pre-training data that captures the abstract attributes of texts humans intuitively perceive. We demonstrate that using state-of-the-art LLMs (e.g.,  GPT -3.5-turbo) can discern these qualities in pairwise judgments and emphasize the importance of balancing quality and diversity. We have created QuRatedPajama, a dataset comprising 260 billion tokens with fine-grained quality ratings, and show that sampling according to these ratings improves perplexity and in-context learning. Second, I present  LESS , a method that effectively estimates data influences for identifying relevant instruction-tuning data for specific applications (a setting we call \u201ctargeted instruction tuning\u201d).  LESS  is efficient, transferrable (we can use a smaller model for data selection), optimizer-aware (working with Adam), and easy to interpret. We show that training on a  LESS -selected 5% of the data can often outperform training on full datasets on diverse downstream tasks.","bio":""},{"type":"seminar","title":"\\"What it can create, it may not understand\\" Studying the Limits of Transformers.","speaker":"Nouha Dziri","affiliation":"Allen Institute for AI","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 09 May 2024","time":" 11:00 - 12:00","abstract":"Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. They only take seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. Yet, these models simultaneously show failures on surprisingly trivial problems. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? Are these errors incidental, or do they signal more substantial limitations?\\nIn an attempt to demystify Transformers, in this talk, I will discuss the limits of LLMs across three different compositional tasks.  Our findings show that although LLMs can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, showing weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. We further show that transformers can often solve multi-step compositional problems by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills.\\nOverall, our findings support the hypothesis that models\u2019 generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.","bio":""},{"type":"seminar","title":"Language modelling for the sake of language modelling","speaker":"Nikos Aletras","affiliation":"University of Sheffield","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 02 May 2024","time":" 11:00 - 12:00","abstract":"The scientific innovation in natural language processing (NLP) is at its fastest pace to date, driven by advances in large language models (LLMs). LLMs power multipurpose chatbots, search engines and coding assistants, unleashing a new era of automation.\\nIn this talk, I will attempt to give you a sense of how and to what extent LLMs learn about language. I will show that they can retain remarkable capabilities even by training them under extreme settings, i.e. to perform tasks that might be completely incomprehensible to or impossible for humans.","bio":""},{"type":"seminar","title":"Large language models for enabling constructive online conversations","speaker":"Kristina Gligori\u0107","affiliation":"Stanford University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 25 April 2024","time":" 16:00 - 17:00","abstract":"NLP  systems promise to disrupt society through applications in high-stakes social domains. However, current evaluation and development focus on tasks that are not grounded in specific societal implications, which can lead to societal harm. There is a need to evaluate and mitigate the societal harms and, in doing so, bridge the gap between the realities of application and how models are currently developed.\\nIn this talk, I will present recent work addressing these issues in the domain of online content moderation. In the first part, I will discuss online content moderation to enable constructive conversations about race. Content moderation practices on social media risk silencing the voices of historically marginalized groups. We find that both the most recent models and humans disproportionately flag posts in which users share personal experiences of racism. Not only does this censorship hinder the potential of social media to give voice to marginalized communities, but we also find that witnessing such censorship exacerbates feelings of isolation. We offer a path to reduce censorship through a psychologically informed reframing of moderation guidelines. These findings reveal how automated content moderation practices can help or hinder this effort in an increasingly diverse nation where online interactions are commonplace.\\nIn the second part, I will discuss how identified biases in models can be traced to the use-mention distinction, which is the difference between the use of words to convey a speaker\u2019s intent and mention of words for quoting what someone said or pointing out properties of a word. Computationally modeling the use-mention distinction is crucial for enabling counterspeech to hate and misinformation. Counterspeech that refutes problematic content mentions harmful language but is not harmful itself. We show that even recent language models fail at distinguishing use from mention and that this failure propagates to downstream tasks. We introduce prompting mitigations that teach the use-mention distinction and show that they reduce these errors.\\nFinally, I will discuss the big picture and other recent efforts to address these issues in different domains beyond content moderation, including education, emotional support, and public discourse about AI. I will reflect on how, by doing so, we can minimize the harms and develop and apply  NLP  systems for social good.","bio":""},{"type":"seminar","title":"Inverting Language Models","speaker":"Alexander (Sasha) Rush","affiliation":"Cornell and Hugging Face","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 14 March 2024","time":" 14:00 - 15:00","abstract":"As language models enter production environments, their intermediate states are used for a myriad of downstream applications such as search, prompting, and document comparison. In this talk, I discuss the feasibility of language model inversion. Specifically, we are interested in how much information language models contain about their inputs? We investigate the problem in two scenarios, recovering text inputs from the outputs of embeddings from sentence embedders and next-token probability outputs from language models. In many cases, our methods are able to fully recover exact textual inputs given just intermediate states. I\u2019ll discuss the security implications of these findings, as well as what this tells us about compression embedding and language modeling applications.","bio":""},{"type":"seminar","title":"Large Language Models in Medicine: Opportunities and Challenges","speaker":"Mark Dredze","affiliation":"Johns Hopkins University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 07 March 2024","time":" 14:00 - 15:00","abstract":"The rapid advance of AI driven by Large Language Models (LLMs), like ChatGPT, has led to impressive results across a range of different use cases. This has included several models developed for the medical domain which have exhibited surprising behaviors, such as answering medical questions and performing well on medical licensing exams. These results have demonstrated the coming transformation of medicine by AI. In this talk, I will provide an overview of some of the recent advances in this area, and discuss challenges and opportunities for the use of these models in medicine.","bio":""},{"type":"seminar","title":"What do sentence transformers know, and how can we find out?","speaker":"Sebastian Pado","affiliation":"Stuttgart University","link":"","venue":"offline","place":"GR06/07 | Faculty of English, 9 West Road, CB3 9DP","date":"Thursday 29 February 2024","time":" 11:00 - 12:00","abstract":"Transformer architectures specialised for producing full-sentence\\nrepresentations, notably  SBERT , often achieve better performance on\\ndownstream tasks than sentence embeddings extracted from vanilla  BERT .\\nHowever, compared to Vanilla transformers, we still have a limited\\nunderstanding of which linguistic properties of the inputs are\\nrepresented well (or less well) within these models.\\nIn my presentation, I will report on two angles from which we have\\nanalyzed  SBERT : (a), a black-box testing approach where we build\\nminimal pairs of synthetic sentences to observe and analyze\\ndifferences in the model\u2019s predictions [1]; and (b), a white-box\\ntesting approach where we extend the Integrated Gradients attribution\\nmethod to the Siamese case. This permits us to decompose model\\npredictions on arbitrary input in terms of the contributions of\\nindividual token pairs [2,3].\\n[1] Dmitry Nikolaev and Sebastian Pad\xf3.\\n    Representation biases in sentence transformers.\\n    Proceedings of  EACL . Dubrovnik, Croatia, 2023.\\n    https://aclanthology.org/2023.eacl-main.268\\n[2] Lucas M\xf6ller, Dmitry Nikolaev and Sebastian Pad\xf3.\\n    An Attribution Method for Siamese Encoders.\\n    Proceedings of  EMNLP . Singapore, 2023.\\n    https://aclanthology.org/2023.emnlp-main.980\\n[3] Lucas M\xf6ller, Dmitry Nikolaev and Sebastian Pad\xf3.\\n    Approximate Attributions for Off-the-Shelf Siamese Transformers.\\n    Proceedings of  EACL . St Julian\u2019s, Malta, 2024.\\n    https://arxiv.org/abs/2402.02883","bio":""},{"type":"seminar","title":"Making Better Use of (Large) Language and Translation Models with Simple Inference Improvements.","speaker":"Rico Sennrich","affiliation":"University of Zurich","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 22 February 2024","time":" 11:00 - 12:00","abstract":"In a field where the state of the art is often advanced by scale \u2013 building larger models on more data \u2013 I will make the argument that a surprising amount of progress can be achieved with simple modifications to inference algorithms. In this talk, I will focus on machine translation, where massively multilingual models and large language models have been shown to handle many translation directions, but which still suffer from problems such as hallucinations or translations in the wrong language. I will show how these issues can be reduced massively with contrastive decoding methods that pair each input with appropriate contrastive inputs. I will also discuss Minimum Bayes Risk (MBR) Decoding, a decoding method that has received renewed interest because it avoids common pitfalls in machine translation, but which suffers from a major increase in computational cost. However, I will show how the computational complexity of  MBR  decoding can be reduced from quadratic to linear to the number of samples by using reference aggregation.","bio":""},{"type":"seminar","title":"Prosocial Language Models","speaker":"Soroush Vosoughi","affiliation":"Dartmouth College","link":"","venue":"offline","place":"GR06/07 | Faculty of English, 9 West Road, CB3 9DP","date":"Thursday 15 February 2024","time":" 11:00 - 12:00","abstract":"Large language models, such as  GPT -4, have marked a significant advancement in the field of natural language processing, achieving near-human performance across a variety of tasks with minimal to no additional training data. The remarkable capabilities of these models can be attributed to their substantial parameter counts, often reaching into the hundreds or thousands of millions, and the extensive datasets sourced from the web for their pre-training. Despite their successes, the very characteristics that empower these models also render them susceptible to mirroring web-based biases and antisocial behaviors. Such reflections pose considerable challenges in deploying these models in real-world scenarios, particularly in socially sensitive applications. In response, our laboratory focuses on developing techniques for the post hoc mitigation of these antisocial tendencies, allowing for the enforcement of prosocial behaviors during model inference without the need for resource-intensive retraining. This presentation will delve into our latest efforts to reduce bias and enhance alignment with human ethical standards in language models through inference-time interventions.","bio":""},{"type":"seminar","title":"Understanding LLMs via their Generative Successes and Shortcomings.","speaker":"Swabha Swayamdipta","affiliation":"University of Southern California","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 08 February 2024","time":" 16:00 - 17:00","abstract":"Generative capabilities of large language models have grown beyond the wildest imagination of the broader AI research community, leading many to speculate whether these successes may be attributed to the training data or different factors concerning the model. I will present some work from my group which has revealed unique successes and shortcomings in the generative capabilities of LLMs, on knowledge-oriented tasks, tasks with human and social utility and tasks that reveal more than surface-level understanding of language. I will also discuss some aspects of language generation itself and why algorithms like truncation sampling have been so successful.","bio":""},{"type":"seminar","title":"Replicating and auditing black-box Language Models.","speaker":"Tatsunori Hashimoto.","affiliation":"","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 25 January 2024","time":" 16:00 - 17:00","abstract":"Advances in large language models have brought about exciting advancements in capabilities, but the commercialization of this technology has led to an increasing loss of transparency. State-of-the-art language models effectively operate as black boxes, with many things unknown about their training algorithms, data annotators, and pretraining data. I will cover a trio of recent works from my group that attempt to help us understand each of these components by replicating the  RLHF  training process (AlpacaFarm), probing LMs to identify whose opinions are being reflected in pretraining and  RLHF  data (OpinionQA), and providing provable guarantees of test set contamination in black-box language models.","bio":""},{"type":"seminar","title":"Can Language Models Learn Truthfulness?","speaker":"He He","affiliation":"New York University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 18 January 2024","time":" 15:00 - 16:00","abstract":"Today\u2019s large language models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? This talk introduces a hypothesis for how LLMs can model truthfulness. Inspired by the agent-model view of language models, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. I will discuss both results on real data and controlled experiments on synthetic data that support the hypothesis. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.","bio":""},{"type":"seminar","title":"Can Language Models Learn Truthfulness?","speaker":"He He","affiliation":"New York University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 18 January 2024","time":" 15:00 - 16:00","abstract":"Today\u2019s large language models (LLMs) are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? This talk introduces a hypothesis for how LLMs can model truthfulness. Inspired by the agent-model view of language models, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. I will discuss both results on real data and controlled experiments on synthetic data that support the hypothesis. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.","bio":""},{"type":"seminar","title":"Assessing the Social Capacity of Large Language Models","speaker":"David Jurgens","affiliation":"University of Michigan","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 30 November 2023","time":" 15:00 - 16:00","abstract":"Much of language is social: nearly all of what we communicate is directed to other people in a particular social context. Yet, our traditional models for natural language understanding have typically operated independently of this social context. With large language models (LLMs) now being used in more interpersonal contexts, we ask to what degree do these models understand the social world in which they operate. In this talk, I will describe three studies aiming to measure the social capacity of LLMs from different angles: their ability to recognize different types of social information in messages, their ability to use the social context to correctly interpret a context-sensitive message, and their potential for innate personality-like characteristics. Through these examples, I will highlight the importance of social information for improving natural language processing models.","bio":""},{"type":"seminar","title":"A sanity check on emergent properties","speaker":"Anna Rogers","affiliation":"IT University of Copenhagen","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 23 November 2023","time":" 11:00 - 12:00","abstract":"One of the frequent points in the mainstream narrative about large language models is that they have \u201cemergent properties\u201d (sometimes even dangerous enough to be considered existential risk to mankind). However, there is much disagreement about even the very definition of such properties. If they are understood as a kind of generalization beyond training data \u2013 as something that a model does without being explicitly trained for it \u2013 I argue that we have not in fact established the existence of any such properties, and at the moment we do not even have the methodology for doing so.","bio":""},{"type":"seminar","title":"Language Understanding for the Political Sciences","speaker":"Simone Paolo Ponzetto","affiliation":"University of Mannheim","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 16 November 2023","time":" 11:00 - 12:00","abstract":"Over the past two decades, political scientists have increasingly adopted and developed natural language processing methods to use text as an additional source of data in their analyses. Over the last decade, the use of computational methods to analyse political texts has expanded significantly, leading to a substantial growth of the text-as-data community within political science.\\nIn this talk, I will present some of our recent contributions at the intersection of document understanding and automated analysis of political texts. I will also argue for the need for increasingly deeper levels of (linguistic) analysis to understand complex phenomena such as ideologies and political attitudes from texts.","bio":""},{"type":"seminar","title":"One Size Does Not Fit All: Towards AI For Everyone","speaker":"Rada Mihalcea","affiliation":"University of Michigan","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 09 November 2023","time":" 11:00 - 12:00","abstract":"Recent years have witnessed remarkable advancements in AI, with language and vision models that have enabled progress in numerous applications and opened the door to the integration of AI in areas such as communication, transportation, healthcare, and arts. Yet, these models mainly adopt a one-size-fits-all strategy, failing to account for individual and group variations.  In this talk, I will show some of the limitations and lack of representation of current AI models, and highlight the need for cross-cultural language and vision models that can capture the diversity of behaviors, beliefs, and language expressions across different groups. I will also explore ways in which we can address these limitations by developing models that are re-centered around people and their unique characteristics.","bio":""},{"type":"seminar","title":"Unhumanizing Models. Why we Need to Change how We Think about AI","speaker":"Dirk Hovy (Bocconi University in Milan","affiliation":"Italy)","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 02 November 2023","time":" 11:00 - 12:00","abstract":"AI models are seemingly everywhere these days, feeling more human than ever before. However, we should be careful not to humanize those models, as it gives them powers they do not possess and obscures the flaws they do have. In this talk, I will look at examples where models seem to know, understand, feel, create, judge, or move like humans and why it is still wrong to anthropomorphize them. At the same time, we will have to find a way to inform AI about what it means to be human if we want to prevent harm and improve their capabilities. For that, we must look across disciplinary boundaries and build a (social) theory of AI.","bio":""},{"type":"seminar","title":"Learning disentangled representation for interpretable language model. / Interactive Narrative Understanding.","speaker":"Lin Gui and Runcong Zhao","affiliation":"King\'s College London","link":"","venue":"offline","place":"GR05, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 19 October 2023","time":" 11:00 - 12:00","abstract":"Recent years have witnessed increasing interest in developing interpretable models in Natural Language Processing (NLP). Most existing models aim at identifying input features such as words or phrases important for model predictions. Neural models developed in  NLP , however, often compose word semantics in a disentangled manner. As such, interpretation by words or phrases only cannot faithfully explain model decisions. In our recent work, we propose a series of disentangle representation learning methods for interpretable language models, including the interpretation in text classification with hierarchical explanation, the uncertainty estimation in prediction, and controllable language generation with disentanglement. Experimental results on real world datasets show that our proposed approaches are able to generate interpretations more faithful to model predictions and better understood by humans.\\nLarge language models (LLMs) can be used to generate human-like responses, offering a promising avenue for creating immersive and interactive environments. These environments have the potential to emulate the dynamic storylines readers might encounter in books, similar to those portrayed in the television series \u201cWestworld.\u201d However, the capability of LLMs to truly grasp an author\u2019s intent remains a challenge. Narrative understanding seeks to capture the cognitive processes of authors, shedding light on their knowledge, intentions, beliefs, and desires.\\nWe will introduce our recent work, NarrativePlay, a system that allows users to role-play a fictional character and interact with other characters in narratives such as novels in an immersive environment, guided by personality traits extracted from narratives. We also incorporate automatically generated visual displays of narrative settings, character portraits, and character speech, greatly enhancing the overall user experience.","bio":""},{"type":"seminar","title":"Mind the Data","speaker":"Noah Smith","affiliation":"University of Washington","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 08 June 2023","time":" 16:00 - 17:00","abstract":"Today\u2019s mainstream  NLP  research focuses on general-purpose models that are scaled up to work with extremely large datasets.  This direction has had many benefits, evidenced by performance on research benchmarks and by new use cases for AI in general, and language models specifically, imagined by an ever wider community of stakeholders. What I believe is coming next is a strong demand for customization. More people than ever will want to adapt language models to create new applications. To enable them, I believe we need new affordances for working with the most important ingredient for  NLP  systems: the data. In this talk, I\u2019ll present recent work from my group showing benefits and risks of new methods for data selection, organization, and synthesis. I\u2019ll advocate for a future in which artifacts like language models are developed to support adaptation to unexpected and diverging demands of a wide population of users, who in turn should be empowered to direct models to serve their own interests.","bio":""},{"type":"seminar","title":"Towards Human-Centered Explanations of AI Predictions","speaker":"Chenhao Tan","affiliation":"University of Chicago","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 01 June 2023","time":" 16:00 - 17:00","abstract":"Explanations of AI predictions are considered crucial for human-AI interactions. I argue that successful human-AI interactions require two steps: AI explanation and human interpretation. Therefore, effective explanations necessitates the understanding of human interpretation. In this talk, I will present our work to address this challenge through human-centered evaluation and generation of explanations. First, I will discuss the distinction between emulation and discovery tasks, which shapes human interpretation. In emulation tasks, humans provide groundtruth labels and the goal of AI is to emulate human intelligence. While it may seem intuitive that humans can provide valid explanations in this case, I argue that humans may not be able to provide \u201cgood\u201d explanations. Caution is thus required to use human explanations for evaluation or as supervision signals despite the growing efforts in building datasets of human explanations. In contrast, in discovery tasks, humans may not necessarily know the groundtruth label. Human-subject experiments show that explanations fail to improve human decisions, namely, human+AI rarely outperforms AI alone. I will highlight the importance of identifying human strengths and AI strengths, and introduce decision-focused summarization. Finally, I will discuss recent work on leveraging explanations to improve AI models.","bio":""},{"type":"seminar","title":"Pretraining, Instruction Tuning, Alignment: Towards Building Large Language Models from First Principles","speaker":"Yao Fu","affiliation":"University of Edinburgh","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 25 May 2023","time":" 11:00 - 12:00","abstract":"Recently, the field has been greatly impressed and inspired by Large Language Models (LLMs). LLMs\u2019 multi-dimensional abilities are significantly beyond many AI researchers\u2019 and practitioners\u2019 expectations and thus reshaping the AI research paradigm. A natural question is how LLMs get there, and where these fantastic abilities come from. In this talk, we try to dissect the strong LLMs\u2019 capabilities and trace them to their sources. We first review the generic recipe for building large language models from first principles. Then we discuss recipes for improving language models\u2019 reasoning capabilities. Finally, we consider further improvements by complexity-based prompting, distilling chain-of-thought, and learning from AI feedback.","bio":""},{"type":"seminar","title":"Probing Language Models for Paraphrastic Representations of Negation and Antonymy","speaker":"Teemu Vahtola","affiliation":"University of Helsinki","link":"","venue":"offline","place":"GR05, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 18 May 2023","time":" 11:00 - 12:00","abstract":"The ability of large language models to represent essential linguistic properties makes them applicable across a wide range of tasks. However, it is still an open question how well they address systematic compositionality and to what level of abstraction they reflect the actual meaning behind a given sentence. I present work on constructing a dedicated challenge set focusing on a specific linguistic phenomenon. In particular, the challenge set assesses the ability of language models to properly represent paraphrasticity realised in expressions that incorporate antonymy and negation.\\nI present a large-scale evaluation of publicly available pre-trained language models on the proposed challenge set, showing large variation between models fine-tuned for different downstream tasks.","bio":""},{"type":"seminar","title":"Writing for the Public: Immigration Discourse and Language Ideology","speaker":"Dallas Card","affiliation":"University of Michigan","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 04 May 2023","time":" 15:00 - 16:00","abstract":"In this talk, I will present some recent methodological and substantive work in text-based computational social science. To begin, I will present work on the history of immigration discourse in American political speech over the past 150 years, emphasizing aspects of tone, framing, and metaphor. Next, I will present a paper from  EMNLP 2022  on the role of language ideology in data curation, specifically with respect to the question of text quality. Finally, time permitting, I will briefly discuss practical approaches to dealing with domain shift in text classification, especially when working with lexicons or otherwise inaccessible models (e.g., commercial APIs).","bio":""},{"type":"seminar","title":"Real-world task assistance with GRILLBot, the winning Amazon Alexa Prize TaskBot.","speaker":"Carlos Gemmell","affiliation":"University of Glasgow","link":"","venue":"offline","place":"SR 24","date":"Thursday 16 March 2023","time":" 11:00 - 12:00","abstract":"Effective virtual assistants have to guide users through long and complex tasks, be engaging, and help solve unpredictable problems along the way. This talk presents  GRILL  Bot, a multi-modal task-oriented assistant, as the winning system in the 2022 Amazon Alexa Prize TaskBot challenge.  GRILL  Bot personalizes assistance in cooking,  DIY  , and crafts by leveraging TaskGraphs, a novel task representation. TaskGrahps unify steps, requirements, and curated domain knowledge into dynamic graphs enabling detailed contextual explanations and adaptable task execution. Further augmentations with segmented videos and linked step images allow full use of the multi-modal experience. We will explore the critical roles language models play in  GRILL  Bot both in offline task augmentations like contextual jokes and in online settings with knowledge-grounded question answering and a semantic parsing approach to dialogue management. We will finally discuss the lessons learned from a full year of real-world users and showcase  GRILL  Bot in action.","bio":""},{"type":"seminar","title":"Towards a Responsible NLP: Walking the walk","speaker":"Mona Diab","affiliation":"Meta AI","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 09 March 2023","time":" 16:00 - 17:00","abstract":"In a world of racing to get the best systems on leaderboards, winning best shared tasks, building the largest LM, are we losing our soul as a scientific enterprise? Do we need to re-orient and re-pivot  NLP ? If so, what is needed to make this happen? Can we chart together a program where we ensure that science is the pivotal ingredient in CL/NLP? Could Responsible  NLP  be an avenue that could lead us back towards that goal? In this talk, in the spirit of EMpirical  NLP , I will explore some \u201cpractical\u201d ideas around framing a Responsible  NLP  vision hoping to achieve a higher scientific standard for our field, addressing issues from the \u201chow\u201d we conduct our research and venturing into the \u201cwhat\u201d we work on and produce using tenets from responsible mindset perspective. I will pose more questions than answers. This is a call to action, an invitation to start a real global community conversation, hopefully engaging all stakeholders: academia, industry, government and civic society.","bio":""},{"type":"seminar","title":"Efficient Transformers with Dynamic Token Pooling","speaker":"Piotr Nawrot","affiliation":"University of Edinburgh","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 02 March 2023","time":" 11:00 - 12:00","abstract":"Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to\\nreduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or\\nphrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is often both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget.","bio":""},{"type":"seminar","title":"One-shot visual language understanding with cross-modal translation and LLMs","speaker":"Fangyu LIU (University of Cambridge).","affiliation":"","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 23 February 2023","time":" 11:00 - 12:00","abstract":"Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. We present the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a  SOTA  model finetuned on thousands of data points, DePlot+LLM with just one-shot prompting achieves a 29.4% improvement over finetuned  SOTA  on human-written queries from the task of chart QA.","bio":""},{"type":"seminar","title":"Challenges and Opportunities in NLP for Under-represented Languages","speaker":"Sebastian Ruder","affiliation":"Google","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 09 February 2023","time":" 11:00 - 12:00","abstract":"Natural language processing (NLP) technology has seen tremendous improvements in recent years but most of these successes have been concentrated in languages with large amounts of data. In this talk, I will discuss challenges and potential solutions on the way to scaling  NLP  to more of the world\u2019s 7000 languages. In particular, I will highlight recent progress in  NLP  for African languages and present methods that are applicable to languages with limited data such as employing alternative sources of data and multi-modal information.","bio":""},{"type":"seminar","title":"Narrative Summarization From Multiple Views","speaker":"Pinelopi (Nelly) Papalampidi","affiliation":"DeepMind","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 02 February 2023","time":" 11:00 - 12:00","abstract":"Although summarizing movies and TV shows comes naturally to humans, it is very challenging for machines. They have to combine different input sources (i.e., video, audio, subtitles), process long videos of 1-2 hours, and their transcripts, and learn from a handful of examples, since collecting and processing such videos is hard. Given the challenges of multimodal summarization, most prior work does not consider all facets of the computational problem at once but instead focuses on either processing multiple but short input sources or long text-only narratives.\\nIn contrast, we aim at summarizing full-length movies and TV episodes while considering all input sources for creating video trailers and textual summaries. For trailer creation, we propose an algorithm for selecting trailer moments in movies based on interpretable criteria such as the narrative importance and sentiment intensity of events. We further demonstrate how we can convert our algorithm into an interactive tool for trailer creation with a human in the loop. Next, for producing textual summaries from full-length TV episodes, we move to a video-to-text setting and hypothesize that multimodal information from the full-length video and audio can directly facilitate abstractive dialogue summarization. We propose a parameter-efficient way for incorporating such information into a pre-trained textual summarizer and demonstrate improvements in the generated summaries.","bio":""},{"type":"seminar","title":"SafetyKit: First Aid for Measuring Safety in Open-domain Conversational Systems","speaker":"Verena Rieser","affiliation":"Heriot-Watt University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 26 January 2023","time":" 11:00 - 12:00","abstract":"The social impact of natural language processing and its applications has received increasing attention. In this position paper, we focus on the problem of safety for end-to-end conversational AI. We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the Instigator, Yea-Sayer, and Impostor effects. We then empirically assess the extent to which current tools can measure these effects and current systems display them. We release these tools as part of a \u201cfirst aid kit\u201d (SafetyKit) to quickly assess apparent safety concerns. Our results show that, while current tools are able to provide an estimate of the relative safety of systems in various settings, they still have several shortcomings. We suggest several future directions and discuss ethical considerations.","bio":""},{"type":"seminar","title":"Revisiting Cross-Lingual Transfer Learning","speaker":"Mikel Artetxe","affiliation":"Meta","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 01 December 2022","time":" 11:00 - 12:00","abstract":"Given downstream training data in one language (typically English), the goal of cross-lingual transfer learning is to perform the task in another language. Existing approaches have been broadly classified into 3 categories: zero-shot (fine-tune a multilingual language model in English and zero-shot transfer into the target language), translate-train (translate the training data into the target language through MT and fine-tune a multilingual language model), and translate-test (translate the evaluation data into English through MT and use an English model). Prior work mostly finds that translate-train performs best followed by zero-shot and translate-test, and focuses on improving multilingual models. In this 3-part talk, we will revisit some of the fundamentals of this problem, challenging the conventional wisdom in the area. First, we will see that a large part of the improvements from using parallel data can be attributed to explicitly modeling parallel interactions, and similar improvements can be obtained using synthetic data. Second, we will revisit the integration of MT into the pipeline, showing that the potential of translate-test has been largely underestimated. Finally, we will see how creating multilingual benchmarks through translation, as it is commonly done, can result in evaluation artifacts, which calls to reconsider some prior findings.","bio":""},{"type":"seminar","title":"End-to-End Fine-grained Multi-modal Understanding","speaker":"Aishwarya Kamath","affiliation":"New York University","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 17 November 2022","time":" 11:00 - 12:00","abstract":"Previously, multi-modal reasoning systems relied on a pre-trained object detector to extract regions of interest from the image. However, this crucial module was typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This made it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this talk, I will first discuss  MDETR , an end-to-end modulated detector that detects objects in an image, conditioned on a raw text query like a caption or a question. The model is trained on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. Next, we will explore further developments in architecture design that employ fusion between the visual and textual modalities deeper in the model, achieving state of the art results when coupled with a coarse-to-fine pre-training strategy. Finally, I will discuss a novel fine-grained visual understanding task and evaluation benchmark which shows that existing benchmarks overestimate VL model\u2019s ability to understand and reason over complex visual scenes leaving substantial room for improvement.","bio":""},{"type":"seminar","title":"Program Synthesis and Understanding with Pretrained Language Models","speaker":"Ignacio Iacobacci","affiliation":"Huawei","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 10 November 2022","time":" 11:00 - 12:00","abstract":"In the last few years there has been a tremendous growth in the topic of understanding and generation using  NLP -grounded deep learning models. While earlier approaches were able to deal with just the simplest tasks, the recent application of Pretrained Language Models (PLMs), specifically trained with code snippets, has brought new capabilities, especially for the task of text-to-code generation or program synthesis. This talk will discuss the reasons of the recent growth of interest on this topic. We will discuss the main differences between working on natural language and programming language. We will provide an overview of the latest approaches, their intended use and limitations. Among other models we will introduce PanguCoder, our brand new Huawei in-house model for code synthesis, which constitutes the building block of the AI-assisted tool for code generation from Huawei. We will cover existing datasets and benchmarks are useful to make a fair comparison among different approaches. We will mention CodeXGlue, a benchmark that cover most common code-oriented tasks, and HumanEval which is, at this time, the de facto benchmark for text-to-code generation. Finally, we will show some applications in the real word and future perspectives in the area","bio":""},{"type":"seminar","title":"Flamingo: a Visual Language Model for Few-Shot Learning","speaker":"Antoine Miech","affiliation":"DeepMind","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 03 November 2022","time":" 11:00 - 12:00","abstract":"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. In this talk, I will introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer, captioning tasks, which evaluate the ability to describe a scene or an event, and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.","bio":""},{"type":"seminar","title":"Contextualized embeddings for lexical semantics","speaker":"Katrin Erk","affiliation":"University of Texas at Austin","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 20 October 2022","time":" 15:00 - 16:00","abstract":"Word embeddings are dense vector representations computed automatically from large amounts of text. From a lexical semantics perspective, we can view an embedding as a compact aggregate of many observed word uses, from different speakers. Especially contextualized word embeddings are highly interesting for lexical semantics because they give us a potential window into garden-variety polysemy: polysemy that is entirely idiosyncratic, not regular. But there is not yet a standardized way to use contextualized embeddings for lexical semantics. I report on two studies we have been doing. In the first, we tested the use of word token clusters on the task of type-level similarity. In the second, we are mapping word token embeddings to human-readable features. I also comment on a trend in word embeddings, from count-based embeddings to the most recent contextualized embeddings, to pick up on what could be called traces of stories: text topics, judgments and sentiment, and cultural trends.  I argue that this is actually an interesting signal and not a bug.","bio":""},{"type":"seminar","title":"Quantifying Linguistic Variation","speaker":"Max M\xfcller-Eberstein","affiliation":"IT University of Copenhagen","link":"","venue":"offline","place":"GR04, English Faculty Building, 9 West Road, Sidgwick Site","date":"Thursday 13 October 2022","time":" 11:00 - 12:00","abstract":"Variation in language is ubiquitous, but difficult to define. Starting from Natural Language Processing\u2019s more pragmatic definition that an increase in distributional shift leads to decreases in transferability, we search for quantitative measures of linguistic variation. By identifying subspaces in latent representations which relate to specific syntactic and semantic features, we attempt to disentangle interpretable dimensions of variation. This segmentation allows us to identify suitable data for transfer learning in specialized low-resource scenarios as well as to accurately evaluate the robustness of pre-trained models before fine-tuning them.","bio":""},{"type":"seminar","title":"Detect \u2013 Verify \u2013 Communicate: Combating Misinformation with More Realistic NLP","speaker":"Iryna Gurevych","affiliation":"TU Darmstadt","link":"","venue":"offline","place":"English Faculty Building, second floor, SR24","date":"Thursday 29 September 2022","time":" 11:00 - 12:00","abstract":"Dealing with misinformation is a grand challenge of the information society directed at equipping the computer users with effective tools for identifying and debunking misinformation. Current Natural Language Processing (NLP) including its fact-checking research fails to meet the expectations of real-life scenarios. In this talk, we show why the past work on fact-checking has not yet led to truly useful tools for managing misinformation, and discuss our ongoing work on more realistic solutions.  NLP  systems are expensive in terms of financial cost, computation, and manpower needed to create data for the learning process. With that in mind, we are pursuing research on detection of emerging misinformation topics to focus human attention on the most harmful, novel examples. Automatic methods for claim verification rely on large, high-quality datasets. To this end, we have constructed two corpora for fact checking, considering larger evidence documents and pushing the state of the art closer to the reality of combating misinformation. We further compare the capabilities of automatic,  NLP -based approaches to what human fact checkers actually do, uncovering critical research directions for the future. To edify false beliefs, we are collaborating with cognitive scientists and psychologists to automatically detect and respond to attitudes of vaccine hesitancy, encouraging anti-vaxxers to change their minds with effective communication strategies.","bio":""},{"type":"seminar","title":"Gold Doesn\'t Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information","speaker":"Yftah Ziser","affiliation":"University of Edinburgh","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 16 June 2022","time":" 11:00 - 12:00","abstract":"We describe a simple and effective method (Spectral Attribute removaL;  SAL ) to remove guarded information from neural representations. Our method uses singular value decomposition and eigenvalue decomposition to project the input representations into directions with reduced covariance with the guarded information rather than maximal covariance, as normally, these factorization methods are used. We begin with linear information removal and proceed to generalize our algorithm to the case of nonlinear information removal using kernels. Our experiments demonstrate that our algorithm retains better main task performance after removing the guarded information compared to previous methods. In addition, our experiments demonstrate that we need a relatively small amount of guarded attribute data to remove information about these attributes, which lowers the exposure to such possibly sensitive data and fits better low-resource scenarios.","bio":""},{"type":"seminar","title":"Cross-lingual Learning, and Applications in Dialog and Translation","speaker":"Junjie Hu","affiliation":"University of Wisconsin-Madison","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 09 June 2022","time":" 15:00 - 16:00","abstract":"Over the last decade, the phenomenal success of  NLP  systems has been mostly driven by deep neural networks and supervised machine learning approaches to a large amount of labeled data. However, it\u2019s infeasible to annotate data under all possible real-world scenarios. As a result, these systems may fail dramatically in practice when dealing with complex textual data written in different languages, or even associated with different data modalities. In this talk, I will present works that are important to extend the generalization ability of  NLP  systems across languages. First, I will present our work on  XTREME  which provides a platform for cross-lingual learning on 9  NLP  tasks in 40 languages. \u200b\u200bI will then demonstrate two cross-lingual applications of task-oriented dialog and machine translation. This talk will be concluded with an overview of my research and some future directions.","bio":""},{"type":"seminar","title":"A Contrastive Framework for Neural Text Generation","speaker":"Yixuan Su","affiliation":"University of Cambridge","link":"","venue":"offline","place":"Board Room, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 02 June 2022","time":" 11:00 - 12:00","abstract":"Text generation is of great importance to many  NLP  applications. However, maximization-based decoding methods (e.g. beam search) of neural language models often lead to degenerate solutions\u2014-the generated text is unnatural and contains undesirable repetitions. Existing approaches introduce stochasticity via sampling or modify training objectives to decrease probabilities of certain tokens (e.g., unlikelihood training). However, they often lead to solutions that lack coherence. In this work, we show that an underlying reason for model degeneration is the anisotropic distribution of token representations. We present a contrastive solution: (i) SimCTG, a contrastive training objective to calibrate the model\u2019s representation space, and (ii) a decoding method\u2014-contrastive search\u2014-to encourage diversity while maintaining coherence in the generated text. Extensive experiments and analyses on three benchmarks from two languages demonstrate that our proposed approach outperforms state-of-the-art text generation methods as evaluated by both human and automatic metrics.","bio":""},{"type":"seminar","title":"Can we automatically anonymize text documents?","speaker":"Pierre Lison","affiliation":"Norwegian Computing Center","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 19 May 2022","time":" 11:00 - 12:00","abstract":"Text documents often contain personal data in some form. To protect the privacy of the individuals referred to in those documents, it is often desirable (and, in many cases, mandatory) to edit those documents such as to conceal the identity of those individuals. This anonymization process remains a difficult task, at the intersection of  NLP , law and data privacy. In this talk, I\u2019ll give an overview of current approaches and outline a number of unsolved problems. Furthermore, I\u2019ll present the Text Anonymization Benchmark (TAB), a new corpus and evaluation framework dedicated to this task.  TAB  contains 1268 court cases from the European Court of Human Rights manually enriched with detailed annotations regarding the personal data expressed in each document.  We hope this new benchmark will inspire  NLP  researchers to work on this challenging but important problem.","bio":""},{"type":"seminar","title":"The acquisition and processing of grammatical structure: insights from deep learning","speaker":"Roger Levy","affiliation":"MIT","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 05 May 2022","time":" 15:00 - 16:00","abstract":"Psycholinguistics and computational linguistics are the two fields most dedicated to accounting for the computational operations required to understand natural language. Today, both fields find themselves responsible for understanding the behaviors and inductive biases of \u201cblack-box\u201d systems: the human mind and artificial neural-network language models (NLMs), respectively. Contemporary NLMs can be trained on a human lifetime\u2019s worth of text or more, and generate text of apparently remarkable grammaticality and fluency. Here, we use NLMs to address questions of learnability and processing of natural language syntax. By testing NLMs trained on naturalistic corpora as if they were subjects in a psycholinguistics experiment, we show that they exhibit a range of subtle behaviors, including embedding-depth tracking and garden-pathing over long stretches of text, suggesting representations homologous to incremental syntactic state in human language processing. Strikingly, these NLMs also learn many generalizations about the long-distance filler-gap dependencies that are a hallmark of natural language syntax, perhaps most surprisingly many \u201cisland\u201d constraints. I conclude with comments on the long-standing idea of whether the departures of NLMs from the predictions of the \u201ccompetence\u201d grammars developed in generative linguistics might provide a \u201cperformance\u201d account of human language processing: by and large, they don\u2019t.","bio":""},{"type":"seminar","title":"Zero-Shot Cross-lingual Transfer for XNLU","speaker":"Milan Gritta","affiliation":"Huawei Noah\u2019s Ark Lab","link":"","venue":"offline","place":"GR06/07, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 17 March 2022","time":" 11:00 - 12:00","abstract":"Task-oriented personal assistants enable people to interact with a host of devices and services using natural language. One of the challenges of making neural dialogue systems available to more users is the lack of training data for all but a few languages. Zero-shot methods try to solve this issue by acquiring task knowledge in a high-resource language such as English with the aim of transferring it to the low-resource language(s). I will present two  ACL  papers from \u201921 and \u201922 that propose a few novel methods for this application/purpose.","bio":""},{"type":"seminar","title":"IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages","speaker":"Emanuele Bugliarello","affiliation":"University of Copenhagen","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 10 March 2022","time":" 11:00 - 12:00","abstract":"Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. In this talk, I will present the Image-Grounded Language Understanding Evaluation benchmark that aims at filling this gap.  IGLUE  brings together\u2014by both aggregating pre-existing datasets and creating new ones\u2014visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target\u2013source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.","bio":""},{"type":"seminar","title":"Spoken Language Understanding, with and without Pre-training","speaker":"Karen Livescu","affiliation":"TTI-Chicago","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 03 March 2022","time":" 15:00 - 16:00","abstract":"Spoken language understanding (SLU) tasks involve mapping from speech audio signals to semantic labels. Given the complexity of such tasks, good performance might be expected to require large labeled datasets, which are difficult to collect for each new task and domain.  Recent work on self-supervised speech representations has made it feasible to consider learning  SLU  models with limited labeled data, but it is not well understood what pre-trained models learn and how best to apply them to downstream tasks. In this talk I will describe recent work that (1) begins to build a better understanding of the information learned by pre-trained speech models and (2) explores a spoken language understanding task, spoken named entity recognition, with limited labeled data.  Along the way we also explore the question of how access to a speech recognizer helps (or doesn\u2019t help) spoken  NER , as well as other ways of improving low-resource spoken  NER  other than using pre-trained models.","bio":""},{"type":"seminar","title":"Hierarchical Interpretation of Neural Text Classification","speaker":"Prof. Yulan He","affiliation":"University of Warwick","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 24 February 2022","time":" 11:00 - 12:00","abstract":"Recent years have witnessed increasing interests in developing interpretable models in  NLP . Most existing models aim at identifying input features such as words or phrases important for model predictions. Neural models developed in  NLP  however often compose word semantics in a hierarchical manner.  Interpretation by words or phrases only thus cannot faithfully explain model decisions. In this talk, I will present our recently proposed Hierarchical Interpretable Neural Text classifier, called Hint, which is able to identify the latent semantic factors and their compositions which contribute to the model\u2019s final decisions. This is often beyond what word-level interpretations could capture. Experimental results on both review datasets and news datasets show that our proposed approach achieves text classification results on par with existing state-of-the-art text classifiers, and generates interpretations more faithful to model predictions and better understood by humans than other interpretable neural text classifiers.","bio":""},{"type":"seminar","title":"Systematic Inequalities in Language Technology Performance across the World\'s Languages","speaker":"Graham Neubig","affiliation":"Carnegie Mellon University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 17 February 2022","time":" 11:00 - 12:00","abstract":"Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of  NLP  methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world\u2019s 6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in  NLP . Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as more linguistic  NLP  tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of  NLP  research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies.","bio":""},{"type":"seminar","title":"Learning the Difference that Makes a Difference with Counterfactually Augmented Data","speaker":"Zachary Lipton","affiliation":"Carnegie Mellon University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 10 February 2022","time":" 15:00 - 16:00","abstract":"Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. Inspired by this literature (and borrowing from it gesturally), we address natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. I will discuss this method, the early results, some conceptual underpinnings of the approach, and some recent follow-up work.","bio":""},{"type":"seminar","title":"Unsupervised Domain Adaptation for Neural Search","speaker":"Nils Reimers","affiliation":"HuggingFace","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 03 February 2022","time":" 11:00 - 12:00","abstract":"After textual information retrieval has stalled for many years, pre-trained transformer networks gave a big performance boost resulting in extremely better search results.  However, so far these approaches require large amount of training data which is seldom available for many use-cases. In this talk, I will start with an overview of different neural search approach. I will then present  BEIR , a benchmark that test neural search methods in an out-of-domain setting. As the benchmark reveals, many architecture are sensitive to domain shifts limiting their usefulness for many real word applications. To overcome this short-coming, we created Generative Pseudo Labeling (GPL), a method that transfers knowledge from slow, but robust architectures, to fast but domain-sensitive approaches, which results in highly improved search quality.","bio":""},{"type":"seminar","title":"Zero-shot learning and out-of-distribution generalization: two sides of the same coin","speaker":"Jonathan Berant","affiliation":"Tel Aviv University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 02 December 2021","time":" 11:00 - 12:00","abstract":"Recent advances in large pre-trained language models have shifted the  NLP  community\u2019s attention to new challenges: (a) training models with zero, or very few, examples, and (b) generalizing to out-of-distribution examples. In this talk, I will argue that the two are intimately related, and describe ongoing (read, new!) work in those directions. First, I will describe a new pre-training scheme for open-domain question answering that is based on the notion of \u201crecurring spans\u201d across different paragraphs. We show this training scheme leads to a zero-shot retriever that is competitive with  DPR  (which trains on thousands of examples), and is more robust w.r.t the test distribution. Second, I will focus on compositional generalization, a particular type of out-of-distribution generalization setup where models need to generalize to structures that are unobserved at training time. I will show that the view that seq2seq models categorically do not generalize to new compositions is false, and present a more nuanced analysis, which elucidates what are the conditions under which models struggle to compositionally generalize.","bio":""},{"type":"seminar","title":"Clinical De-Identification and Semantic Relatedness","speaker":"Mohamed Abdalla","affiliation":"University of Toronto","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 25 November 2021","time":" 15:00 - 16:00","abstract":"The first part of this talk will discuss the development of novel low-cost approaches to de-identifying clinical notes. The second part of the talk discuss the development of a new dataset of semantic relatedness for sentence pairs.. This dataset,  STR -2021, has 5,500 English sentence pairs manually annotated for semantic relatedness using a comparative annotation framework. We show that the resulting scores have high reliability (repeat annotation correlation of 0.84). We use the dataset to explore a number of questions on what makes two sentences more semantically related. We also evaluate a suite of sentence representation methods on their ability to place pairs that are more related closer to each other in vector space.","bio":""},{"type":"seminar","title":"Supervising Robot Learning with Language and Video from the Web","speaker":"Suraj Nair","affiliation":"Stanford University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 18 November 2021","time":" 15:00 - 16:00","abstract":"While deep reinforcement learning applied to robotics has seen a number of recent successes in constrained environments, generalist robots that can operate in diverse real world settings have remained out of reach. Critically, robot learning algorithms have yet to be able to learn from a sufficient breadth of data that can enable broad generalization across tasks and environments. In this talk, I\u2019ll discuss the paradigm of offline learning for robotics as a path towards generalist robots, and how we might supervise this offline learning process in a scalable way using crowdsourced language and videos of humans. Specifically, I\u2019ll cover two recent papers which learn reward functions for offline reinforcement learning through language annotations of pre-collected robot datasets and human video datasets which exist on the web.","bio":""},{"type":"seminar","title":"Document Summarisation: Modelling, Datasets and Verification of Content","speaker":"Shay Cohen","affiliation":"University of Edinburgh","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 11 November 2021","time":" 11:00 - 12:00","abstract":"Within Natural Language Processing, document summarisation is one of the central problems. It has both short-term societal implications and long-term implications in terms of the success of AI. I will describe advances made in this area with respect to three different aspects: methodology and modelling, dataset development and enforcing factuality of summaries. In relation to modelling, I will show how reinforcement learning can be used to directly maximise the metric by which the summaries are being evaluated. With regards to dataset development, I will describe a dataset that we released for summarisation, XSum, in which a single sentence is used to describe the content of a whole article. The dataset has become a standard benchmark for summarisation. Finally, in relation to factuality, I will show how one can improve the quantitative factuality of summaries by re-ranking them in a beam based on a \u201cverification\u201d model.","bio":""},{"type":"seminar","title":"Measuring Factuality in Text Generation: When Language Models Are Twisting the Facts","speaker":"Roee Aharoni","affiliation":"Google Research","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 28 October 2021","time":" 11:00 - 12:00","abstract":"Text generation is at the core of many  NLP  tasks like question answering, dialog generation, machine translation or text summarization. While current text generation models produce text that seems fluent and informative, their outputs often contain factual inconsistencies with respect to the inputs they rely on (a.k.a. \u201challucinations\u201d), making it hard to deploy such models in real-world applications.\\nIn this talk I will present two of our recent works tackling those issues. First, I will describe  KOBE  (Gekhman et al.,  EMNLP  Findings 2020), a knowledge-based approach for evaluating the quality of machine translation models, which uses multilingual entity resolution instead of human reference translations. I will then present Q^2 (Honovich et al.,  EMNLP 2021 ), an automatic evaluation metric that combines question generation, question answering and natural language inference to validate the outputs of dialogue generation models.","bio":""},{"type":"seminar","title":"Causal analysis of the syntactic representations of Transformers","speaker":"Tal Linzen","affiliation":"New York University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 21 October 2021","time":" 15:00 - 16:00","abstract":"The success of artificial neural networks in language processing tasks has underscored the need to understand how they accomplish their behavior, and, in particular, how their internal vector representations support that behavior. The probing paradigm, which has often been invoked to address this question, relies on the (typically implicit) assumption that if a classifier can decode a particular piece of information from the model\u2019s intermediate representation, then that information plays a role in shaping the model\u2019s behavior. This assumption is not necessarily justified. Using the test case of everyone\u2019s favorite syntactic phenomenon \u2013 English subject-verb number agreement \u2013 I will present an approach that provides much stronger evidence for the causal role of the encoding of a particular linguistic feature in the model\u2019s behavior. This approach, which we refer to as AlterRep, modifies the internal representation in question such that it encodes the opposite value of that feature; e.g., if  BERT  originally encoded a particular word as occurring inside a relative clause, we modify the representation to encode that it is not inside the relative clause. I will show that the conclusions of this method diverge from those of the probing method. Finally, I will present a method based on causal mediation analysis that makes it possible to draw causal conclusions by applying counterfactual interventions to the inputs, contrasting with AlterRep which intervenes on the model\u2019s internal representations.","bio":""},{"type":"seminar","title":"Unlikelihood-training and Back-training for robust natural language understanding","speaker":"Siva Reddy","affiliation":"McGill University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 14 October 2021","time":" 14:00 - 15:00","abstract":"Language models are known to be good at generalization and memorization. These abilities mean that a language model can be directly be used as a knowledge base, e.g., a language model could easily fill the blank in the sentences \u201cThe capital of Canada is  BLANK \u201d and \u201cBLANK is the capital of Canada;\u201d with Ottawa, even if these exact syntactic constructions are never seen during training, a task that requires both generalization and memorization. But we also observe that complex phenomena such as negation are commonly ignored by language models, e.g., the model would still predict Ottawa as the answer to \u201cThe capital of Canada is not  BLANK \u201d. I will introduce a new training procedure and objective called \u201cunlikelihood training with reference\u201d in order to build language models that understand negation.\\nIn the second part of the talk, I will show that pretrain and fine-tune paradigm breaks in an out-of-distribution setting. For example, question answering and generation models trained on Natural Questions do not generalize to other domains such as education or bio-medical. I will introduce a new technique called back-training that exploits unsupervised data in the target domains much more efficiently than self-training.","bio":""},{"type":"seminar","title":"Towards Knowledge-Robust and Multimodally-Grounded NLP","speaker":"Mohit Bansal","affiliation":"UNC Chapel Hill","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 10 June 2021","time":" 17:00 - 18:00","abstract":"In this talk, I will present our group\u2019s work on  NLP  models that are knowledge-robust and multimodally-grounded. First, we will describe multi-task and reinforcement learning methods to incorporate novel auxiliary-skill tasks such as saliency, entailment, and back-translation validity (including bandit-based methods for automatic auxiliary task selection+mixing and multi-reward mixing). Next, we will discuss developing adversarial robustness against reasoning shortcuts, missing commonsense gaps, and cross-domain/lingual generalization in QA and dialogue models (including auto-adversary generation). Lastly, we will discuss multimodally-grounded models which condition and reason on dynamic spatio-temporal information in images and videos, and action-based robotic navigation and assembling tasks (including commonsense reasoning for ambiguous robotic instructions).","bio":""},{"type":"seminar","title":"Learning across Adverse Conditions in Natural Language Processing","speaker":"Barbara Plank","affiliation":"IT University of Copenhagen","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 27 May 2021","time":" 11:00 - 12:00","abstract":"Transferring knowledge to solve a related problem and learning from limited, unreliable inputs are examples of extraordinary human ability. State-of-the-art machine learning models based on deep learning often fail under such adverse conditions. How can we build Natural Language Processing technology which transfer better to new conditions, such as learning to process a new language or a new text domain? Transfer learning (TL) and multi-task learning (MTL) can help remedy this problem. In this talk, I will discuss TL and  MTL  methods to tackle this challenge and present some of our (on-going) work on  NLP  for zero-shot and few-shot transfer, including Danish, a case study on a very low-resource dialect and recent work on information extraction for task-oriented dialogue.","bio":""},{"type":"seminar","title":"Training for Deployment: Methods for Small and Efficient NLP","speaker":"Alexander Rush","affiliation":"Cornell Tech","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 20 May 2021","time":" 15:00 - 16:00","abstract":"Natural language models for translation and classification work relatively well, or at least well enough that there is demand for widespread use in real systems. Models developed for research however do not naturally translate to deployment scenarios, particularly on resource constrained devices like mobile phones. In this talk I will discuss two axes that make it difficult to deploy  NLP  models in practice: a) Serial generation in translation models makes them difficult to optimize, and b) Fine-tuned parameter size in classification makes models difficult to deploy to end-users. I propose two approaches that aim to circumvent these issues, and discuss some practical work on deploying large  NLP  models on edge devices.","bio":""},{"type":"seminar","title":"NLP beyond English and mBERT\'s 100 languages","speaker":"Antonios Anastasopoulos","affiliation":"George Mason University","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 13 May 2021","time":" 15:00 - 16:00","abstract":"The availability of large multilingual pre-trained language models has opened up exciting pathways for developing  NLP  technologies for languages with scarce resources. In this talk I will summarize some of my recent work (a) on the need to go beyond the most common languages in multilingual evaluation, (b) on probing multilingual models for factual knowledge in multiple languages, and\xa9 on the challenges of handling new, unseen languages through finetuning.","bio":""},{"type":"seminar","title":"CausaLM: Causal Model Explanation Through Counterfactual Language Models","speaker":"Amir Feder","affiliation":"Technion - Israel Institute of Technology","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 06 May 2021","time":" 11:00 - 12:00","abstract":"Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all ML-based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as  BERT  can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.","bio":""},{"type":"seminar","title":"Domain Adaptation in NLP - Towards Adaptation to Any Domain","speaker":"Eyal Ben-David","affiliation":"Technion - Israel Institute of Technology","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 29 April 2021","time":" 11:00 - 12:00","abstract":"Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. In this work, we address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from an unseen domain that is unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present  PADA : A Prompt-based Autoregressive Domain Adaptation algorithm, based on the T5 model. Given a test example,  PADA  first generates a unique prompt and then, conditioned on this prompt, labels the example with respect to the  NLP  task. The prompt is a sequence of unrestricted length, consisting of pre-defined Domain Related Features (DRFs) that characterize each of the source domains. Intuitively, the prompt is a unique signature that maps the test example to the semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios,  PADA  substantially outperforms strong baselines.","bio":""},{"type":"seminar","title":"Transcending Dependencies","speaker":"Martha Palmer","affiliation":"University of Colorado","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 22 April 2021","time":" 17:00 - 18:00","abstract":"This talk will discuss symbolic representations of sentences in context, ranging from universal dependencies to abstract meaning representations (AMR), and examine their capability for capturing certain aspects of meaning. A main focus will be the ways in which  AMR \u2019s can be expanded to encompass figurative language, the recovery of implicit arguments and relations between events. These examples will be primarily in English, and indeed some features of  AMR  are fairly English-centric. The talk will conclude by introducing Uniform Meaning Representations, a multi-sentence annotation scheme that is revising  AMR \u2019s to make them more suitable for other languages, especially low resource languages, and expanding the annotation guidelines to include Number, Tense, Aspect and Modality as well as Temporal Relations.","bio":""},{"type":"seminar","title":"Learning language by observing the world and learning about the world from language","speaker":"Aida Nematzadeh","affiliation":"DeepMind","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 18 March 2021","time":" 11:00 - 12:00","abstract":"Children learn about the visual world from implicit\\nsupervision that language provides. Most children learn their\\nlanguage, at least to some extent, by observing the world. Recently released datasets of instructional videos are interesting as they can be considered a rough approximation of a child\u2019s visual and linguistic experience\u2014in these videos, the narrator performs a high-level task (e.g., cooking pasta) while describing the steps involved in that task (e.g., boiling water). Moreover, these datasets pose challenges similar to those children need to address; for example, identifying relevant activities to the task (e.g., boiling water) and ignoring the rest (e.g., shaking head). I will present two projects where we study the interaction of visual and linguistic signals in these videos: (1) We show that using language and the structure of tasks is important in discovering action boundaries. (2) I will discuss how visual signal improves the quality of unsupervised word translation, especially for dissimilar languages, and where we do not have access to large corpora.","bio":""},{"type":"seminar","title":"Achieving Universality in Machine Translation: M4 - Massively Multilingual, Massive MT Models for the Next 1000 Languages","speaker":"Orhan Firat","affiliation":"Google Research","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 11 March 2021","time":" 11:00 - 12:00","abstract":"What does universality mean for machine translation? Massively multilingual models jointly trained on hundreds of languages, have been showing great success in processing different languages simultaneously in a single large model. These large multilingual models, which we call M4, are appealing for both efficiency and positive cross-lingual transfer: (1) Training and deploying a single multilingual model requires much less resources than maintaining one model for each language considered, (2) by transferring knowledge from high-resource languages, multilingual models are able to improve performance on low-resource languages. In this talk, we will be talking about our efforts on scaling machine translation models to more than 1000 languages. We will be detailing several research (and even some development) challenges that the project has tackled; multi-task learning with hundreds of tasks, learning under heavy data imbalance, understanding the learned representations, evaluation at the tail, cross-lingual down-stream transfer and many more insights will be shared.","bio":""},{"type":"seminar","title":"Rethinking Benchmarking in AI","speaker":"Douwe Kiela","affiliation":"Facebook AI Research","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 04 March 2021","time":" 17:00 - 18:00","abstract":"The current benchmarking paradigm in AI has many issues: benchmarks saturate quickly, are susceptible to overfitting, contain exploitable annotator artifacts, have unclear or imperfect evaluation metrics, and do not measure what we really care about. I will talk about my work in trying to rethink the way we do benchmarking in AI, specifically in natural language processing, focusing mostly on the recently launched Dynabench platform.","bio":""},{"type":"seminar","title":"Scalable Structural Inductive Biases in Neural Language Models","speaker":"Adhiguna Kuncoro","affiliation":"DeepMind","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 25 February 2021","time":" 11:00 - 12:00","abstract":"Scalable language models like  BERT  and  GPT -3 have achieved remarkable success in various natural language understanding benchmarks, including on challenging benchmarks of structural competence. Does this success mean that data scale and large models are all we need to fully comprehend natural language? Or can these scalable models instead still benefit from more explicit structural inductive biases?\\nThis talk provides evidence for the latter: We improve the performance of  LSTM  and Transformer models by augmenting them with structural inductive biases derived from an explicitly hierarchical\u2014-albeit harder to scale\u2014-recurrent neural network grammars (RNNG). I will begin with an overview of the proposed structure distillation objective for autoregressive language modelling with  LST Ms. I will then discuss an extension to the masked language modelling case, by distilling the approximate posterior distributions of the  RNNG  teacher, which culminates in structure-distilled  BERT  models that outperform the standard  BERT  model on a diverse suite of structured prediction tasks.\\nAltogether, these findings demonstrate the benefits of syntactic biases, even in scalable language models that learn from large amounts of data, and contribute to a better understanding of where syntactic biases are most helpful in benchmarks of natural language understanding.","bio":""},{"type":"seminar","title":"Efficient sentence encoders for Conversational AI in the industry","speaker":"Inigo Casanueva","affiliation":"PolyAI","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 18 February 2021","time":" 11:00 - 12:00","abstract":"Building real-world conversational AI applications requires resource-efficient models that can learn in low-data regimes with only a handful of annotated examples. Fully fine-tuning large pretrained language models is expensive and computationally intractable for these applications, where fast-paced development cycles are necessary. This talk presents ConveRT and ConVEx; effective, affordable, quick-to-train, and quick-to-fine-tune sentence encoders that work well in such few-shot low-data scenarios. These encoders achieve state-of-the-art performance across a wide range of conversational tasks such as response selection, intent classification and value extraction,  offering quick and effective adaptation to new tasks, domains, and languages.","bio":""},{"type":"seminar","title":"Adapters in Transformers. A New Paradigm for Transfer Learning\u2026?","speaker":"Jonas Pfeiffer","affiliation":"Technical University of Darmstadt","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 11 February 2021","time":" 11:00 - 12:00","abstract":"Adapters have recently been introduced as an alternative transfer learning strategy. Instead of fine-tuning all weights of a pre-trained transformer-based model, small neural network components are introduced at every layer. While the pre-trained parameters are frozen, only the newly introduced adapter weights are fine-tuned, achieving an encapsulation of the down-stream task information in designated parts of the model. In this talk we will provide an introduction to adapter-training in natural language processing. We will go into detail on how the encapsulated knowledge can be leveraged for compositional transfer learning, as well as cross-lingual transfer. We will briefly touch the efficiency of adapters in terms of trainable parameters as well as (wall-clock) training time. Finally, we will provide an outlook to recent alternative adapter approaches and training strategies.","bio":""},{"type":"seminar","title":"From Translation Divergences to Structure-aware Neural Machine Translation","speaker":"Omri Abend","affiliation":"The Hebrew University of Jerusalem","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 04 February 2021","time":" 11:00 - 12:00","abstract":"Languages present a wide range of structures for expressing similar\\nmeanings. This variation has historically motivated the integration of\\nlinguistic structure into machine translation (MT) models, so as to\\nabstract away from realization differences, but such integration has\\nbeen receiving less attention since the introduction of neural MT\\nmodels. In my talk I will discuss ongoing work we\u2019re carrying out in\\nthe lab, on characterizing divergences and their impact on the\\nperformance in today\u2019s neural MT models, as well as on two approaches\\nfor integrating syntactic structure into MT models to address this\\ngap.\\nJoint work with many lab members and collaborators, notably Leshem\\nChoshen, Dmitry Nikolaev, Asaf Yehudai and Lior Fox.","bio":""},{"type":"seminar","title":"Towards explainable fact checking","speaker":"Prof. Isabelle Augenstein","affiliation":"University of Copenhagen","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 28 January 2021","time":" 11:00 - 12:00","abstract":"Automatic fact checking is one of the more involved  NLP  tasks currently researched: not only does it require sentence understanding, but also an understanding of how claims relate to evidence documents and world knowledge. Moreover, there is still no common understanding in the automatic fact checking community of how the subtasks of fact checking \u2014 claim check-worthiness detection, evidence retrieval, veracity prediction \u2014 should be framed. This is partly owing to the complexity of the task, despite efforts to formalise the task of fact checking through the development of benchmark datasets. This talk will re-examine how fact checking is defined, and present some of my recent work on training explainable fact checking models to expose some of the reasoning processes these models follow.","bio":""},{"type":"seminar","title":"Towards Perfect Supervised and Unsupervised Machine Translation","speaker":"Prof. Dr. Alexander Fraser","affiliation":"CIS, LMU Munich","link":"https://cam-ac-uk.zoom.us/j/97599459216?pwd=QTRsOWZCOXRTREVnbTJBdXVpOXFvdz09","venue":"online","place":"","date":"Thursday 21 January 2021","time":" 11:00 - 12:00","abstract":"Data-driven Machine Translation is an interesting application of\\nmachine-learning-based natural language processing techniques to\\nmultilingual data. Particularly with the recent advent of powerful\\nneural network models, it has become possible to incorporate many\\ntypes of information directly into the model and to robustly model\\nlong-distance dependencies in the sequence of words being generated.\\nI will discuss four areas of work addressing important weaknesses of\\ndata-driven machine translation approaches. First, I will present an\\nalternative model to phrase-based statistical machine translation,\\nwhich jointly models translation operations and reordering operations\\nand was widely adopted by researchers and end-users. Second, I will\\ndiscuss the important problem of data sparsity in translation which is\\ncaused by rich morphology, and discuss extensive work we have carried\\nout to overcome this. Third, I will discuss progress towards breaking\\nthe strong domain dependency between the data used to train supervised\\nneural machine translation systems and the data that will be\\ntranslated. Finally, I will briefly present a new research program\\nwhich will allow us to build strong unsupervised machine translation\\nsystems, enabling the carrying out of high quality translation between\\npairs of languages for which no known source of parallel training data\\nexists.","bio":""},{"type":"seminar","title":"Pitfalls in Evaluation of Multilingual Text Representations","speaker":"Goran Glava\u0161.","affiliation":"","link":"","venue":"offline","place":"https://teams.microsoft.com/l/meetup-join/19%3ameeting_YmQyY2ViNDgtZDE1MC00MzZhLWFjZGItOWFmMjM2OTI1ZDQy%40thread.v2/0?context=%7b%22Tid%22%3a%2249a50445-bdfa-4b79-ade3-547b4f3986e9%22%2c%22Oid%22%3a%2230bfe2fc-8896-487c-84f2-f4b8875a60b2%22%7d","date":"Thursday 03 December 2020","time":" 11:00 - 12:00","abstract":"Multilingual representation spaces, spanned by multilingual word embeddings or massively multilingual transformers, conceptually enable modeling of meaning across a wide range of languages and language transfer of task-specific  NLP  models from resource-rich to resource-lean languages. It is not yet clear, however, to which extent this conceptual promise holds in practice. Recent models, both cross-lingual word embedding models and multilingual transformers, have been praised for being able to induce multilingual representation spaces without any explicit supervision (i.e., without any word-level alignments or parallel corpora). In this talk, I will point to some prominent shortcomings and pitfalls of existing evaluations of multilingual representation spaces, which mask important limitations of state-of-the-art multilingual representation models. Remedying for some of these evaluation shortcomings, portrays meaning representation and language transfer capabilities of current state-of-the-art multilingual representation spaces in a less favorable light.","bio":""},{"type":"seminar","title":"From Sparse Modeling to Sparse Communication","speaker":"Andr\xe9 F. T. Martins.","affiliation":"","link":"","venue":"offline","place":"https://teams.microsoft.com/l/meetup-join/19%3ameeting_YmQyY2ViNDgtZDE1MC00MzZhLWFjZGItOWFmMjM2OTI1ZDQy%40thread.v2/0?context=%7b%22Tid%22%3a%2249a50445-bdfa-4b79-ade3-547b4f3986e9%22%2c%22Oid%22%3a%2230bfe2fc-8896-487c-84f2-f4b8875a60b2%22%7d","date":"Thursday 26 November 2020","time":" 11:00 - 12:00","abstract":"Sparse modeling is an important, decades-old area in machine learning which aims to select and discover the relevant features that should be included in a model. In this talk I will describe how this toolbox can be extended and adapted for facilitating sparse communication in neural networks. The building block is a family of sparse transformations called alpha-entmax, a drop-in replacement for softmax. Entmax transformations are differentiable and (unlike softmax) they can return sparse probability distributions, useful to select relevant input features.\\nIn the first part, I will illustrate the use of alpha-entmax in attention mechanisms. These sparse transformations and their structured and continuous variants have been applied with success to machine translation, natural language inference, visual question answering, and other tasks. I will show how learning the alpha parameter can lead to \u201cadaptively sparse transformers,\u201d where each attention head learns to choose between focused or spread-out behavior. I will proceed to  describe a framework for model prediction explainability as a sparse communication problem between an explainer and a layperson, which takes advantage of the selection capabilities of sparse attention. If time permits, I will show how this framework can be extended to continuous domains to obtain sparse densities, illustrating with an application in visual question answering where \u201ccontinuous attention\u201d selects elliptical regions in the image.\\nIn the second part, I will show how sparse transformations can also be used as a replacement for the cross-entropy loss, via the family of entmax losses. This leads to sparse sequence-to-sequence models, where beam search can be exact, and to language models that are natively sparse, eliminating the need for top-k and nucleus sampling. I will show applications in morphological tasks, machine translation, and text generation.\\nThis work was funded by the DeepSPIN  ERC  project (https://deep-spin.github.io).","bio":""},{"type":"seminar","title":"Understanding Event Processes in Natural Language","speaker":"Muhao Chen.","affiliation":"","link":"","venue":"offline","place":"https://teams.microsoft.com/l/meetup-join/19%3ameeting_NzU4Y2NiOTItYjE1My00ZDFhLWE3NTItOWVmMDU1MmNkNjU5%40thread.v2/0?context=%7b%22Tid%22%3a%2249a50445-bdfa-4b79-ade3-547b4f3986e9%22%2c%22Oid%22%3a%227c409a60-a41c-43c1-a95a-f00471773d03%22%7d","date":"Thursday 19 November 2020","time":" 16:00 - 17:00","abstract":"Human languages evolve to communicate about real-world events. Therefore, understanding events plays a critical role in natural language understanding (NLU). A key challenge to this mission lies in the fact that events are not just simple, standalone predicates. Rather, they are often described at different granularities, form different temporal orders, and directed by specific central goals in the context. This talk will present two parts of our recent studies on Event-centric  NLU . In the first part, I will talk about how logically constrained learning can help teach machines to understand temporal relations,\\nmembership relations and coreference of events (e.g., what should be the right process of \u201cdefend a dissertation\u201d, \u201ctaking courses\u201d, \u201cpublish papers\u201d regarding \u201cearning a PhD\u201d?). The second part will talk about how to teach machines to understand the intents and central goals behind event processes (e.g, do machines understand that \u201c making a dough\u201d, \u201cadding toppings\u201d, \u201cpreheating the oven\u201d and \u201cbaking the dough\u201d lead to \u201ccooking pizza\u201d?). I will also briefly discuss some recent advances and open problems in Event-centric  NLU , along with a system demonstration.","bio":""},{"type":"seminar","title":"Semiparametric Language Models","speaker":"Dani Yogatama.","affiliation":"","link":"","venue":"offline","place":"https://teams.microsoft.com/l/meetup-join/19%3ameeting_NjYzNzQyOWUtZWM0MC00YzI5LWEwMzMtOGU0ODM4ZDU1NDcw%40thread.v2/0?context=%7b%22Tid%22%3a%2249a50445-bdfa-4b79-ade3-547b4f3986e9%22%2c%22Oid%22%3a%227c409a60-a41c-43c1-a95a-f00471773d03%22%7d","date":"Thursday 12 November 2020","time":" 14:00 - 15:00","abstract":"Machine learning models work well on a dataset given enough training examples, but they often fail to isolate and reuse previously acquired knowledge when the data distribution shifts (e.g., when presented with a new dataset or very long context). In contrast, humans are able to learn incrementally and accumulate persistent knowledge to facilitate faster learning of new skills without forgetting old ones.\\nIn this talk, I will argue that obtaining such an ability for a language model requires significant advances in how to represent, store, and reuse knowledge acquired from textual data. I will present a semiparametric language model framework that separates computation (information processing) in a large parametric neural network and memory storage in a non-parametric component. I will show two instantiations of such a model. First, I will discuss how to use it to allow a language model to continually learn new tasks without forgetting old ones. Second, I will present a language model architecture that adaptively combines local context and global context to make more accurate predictions.","bio":""},{"type":"seminar","title":"Variational Smoothing in Recurrent Neural Network Language Models","speaker":"Dr. Lingpeng Kong (DeepMind).","affiliation":"","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 27 February 2020","time":" 11:00 - 12:00","abstract":"In this talk, we present a new theoretical perspective of data noising in recurrent neural network language models (Xie et al., 2017). We show that each variant of data noising is an instance of Bayesian recurrent neural networks with a particular variational distribution (i.e., a mixture of Gaussians whose weights depend on statistics derived from the corpus such as the unigram distribution). We use this insight to propose a more principled method to apply at prediction time and propose natural extensions to data noising under the variational framework. In particular, we propose variational smoothing with tied input and output embedding matrices and an element-wise variational smoothing method. We empirically verify our analysis on two bench-mark language modeling datasets and demonstrate performance improvements over existing data noising methods.","bio":""},{"type":"seminar","title":"Developing technologies for health, mental health and wellbeing","speaker":"Rafael A. Calvo","affiliation":"Imperial College London","link":"","venue":"offline","place":"GR04, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 13 February 2020","time":" 11:00 - 12:00","abstract":"A joint Leverhulme Centre for the Future of Intelligence (CFI) and Language Technology Lab (LTL) seminar on Human-Centric AI Technologies\\nEngineers and computer scientists are increasingly faced with philosophical questions asked during the first industrial revolution. Such questions come to the fore when we build health technologies, or use  NLP  in different contexts. Taking them into account is becoming an essential part of our work.  Among the challenges to success is achieving the deep interdisciplinarity involved, which often requires continuous collaboration among medical professionals, psychologists,  HCI  researchers, user experience designers, software developers and end-users.  While some projects lack theoretical grounding or an evidence-base, others fail to involve users effectively in order to understand their needs, perceptions and contexts, resulting in technologies that go unused. \\nWorking together, researchers in  HCI , health and the social sciences can improve the processes by which digital technologies are developed and distributed for the benefit of population-wide health and wellbeing.  In this presentation, I will share some of the multidisciplinary evidence-based approaches to the development of health technologies formerly taken at the Wellbeing Technologies Lab in Sydney, Australia, and now starting at Imperial college London.  I will describe some ideas on the contributions  HCI  can make to this field, and share a number of case studies in the domains of chronic illness, sleep, mental health and doctor-patient communication.","bio":""},{"type":"seminar","title":"Computational Models of the Influence of Context on Sentence Acceptability","speaker":"Shalom Lappin","affiliation":"University of Gothenburg","link":"","venue":"offline","place":"GR04, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 06 February 2020","time":" 11:00 - 12:00","abstract":"We study the influence of context on sentence acceptability. First we compare the crowd source acceptability\\nratings of sentences judged in isolation, with a relevant context, and with an irrelevant context. Our results\\nshow that context induces a cognitive load for humans, which compresses the distribution of ratings. Moreover,\\nin relevant contexts we observe a discourse coherence effect which uniformly raises acceptability. We then\\ntest unidirectional and bidirectional neural language models for their ability to predict acceptability ratings.\\nThe bidirectional models give very promising results, with the best model achieving a new state-of-the-art\\nfor unsupervised acceptability prediction. The two sets of experiments provide insights into the cognitive\\naspects of sentence processing, and central issues in the computational modelling of text and discourse. (Joint work with Jey Han Lau, The University of Melbourne; Carlos Armendariz, Queen Mary University of London; Matthew Purver, Queen Mary University of London; and Chang Shu,University of Nottingham Ningbo China)","bio":""},{"type":"seminar","title":"Urban Dictionary Embeddings for Slang NLP Applications","speaker":"Dr Barbara McGillivray (University of Cambridge and The Alan Turing Institute).","affiliation":"","link":"","venue":"offline","place":"GR04, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 30 January 2020","time":" 11:00 - 12:00","abstract":"The choice of the corpus on which word embeddings are trained can have a sizable effect on the learned representations, the types of analyses that can be performed with them, and their utility as features for machine learning models. In this talk I will present my work on the first set of word embeddings trained on the content of Urban Dictionary, a crowd-sourced dictionary for slang words and phrases. I will show that although these embeddings are trained on fewer total tokens, they have high performance across a range of common word embedding evaluations, ranging from semantic similarity to word clustering tasks. Further, for some extrinsic tasks such as sentiment analysis and sarcasm detection where we expect to require some knowledge of colloquial language on social media data, initializing classifiers with the Urban Dictionary Embeddings resulted in improved performance compared to initializing with a range of other well-known, pre-trained embeddings that are order of magnitude larger in size.","bio":""},{"type":"seminar","title":"Hidden Biases. Ethical Issues in NLP, and What to Do about Them","speaker":"Dirk Hovy","affiliation":"Bocconi University in Milan, Italy","link":"","venue":"offline","place":"GR04, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 23 January 2020","time":" 11:00 - 12:00","abstract":"A joint Leverhulme Centre for the Future of Intelligence (CFI) and Language Technology Lab (LTL) seminar on Human-Centric AI Technologies\\nTexts reflect the authors\u2019 demographic properties and biases, which in turn get magnified by statistical  NLP  models. This has unintended consequences for our analysis: if we do not pay attention to the biases contained, we can easily draw the wrong conclusions, and create disadvantages for our users.\\nIn this talk, I will discuss several types of biases that affect  NLP  models, what their sources are, and potential counter measures. \\n- bias stemming from data, i.e., selection bias (if our texts do not adequately reflect the population we want to study), label bias (if the labels we use are skewed), and semantic bias (the latent stereotypes encoded in embeddings). \\n- biases deriving from the models themselves, i.e., their tendency to amplify any imbalances that are present in the data.\\n- design bias, i.e., the biases arising from our (the researchers) decisions which topics to analyze, which data sets to use, and what to do with them.\\nFor each bias, I will provide examples and discuss the possible ramifications for a wide range of applications, and who various ways to address and counteract these biases, ranging from simple labeling considerations to new types of models.","bio":""},{"type":"seminar","title":"Correlations Between Word Vector Sets","speaker":"Aleksandar Savkov","affiliation":"Babylon","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 16 January 2020","time":" 11:00 - 12:00","abstract":"A joint Leverhulme Centre for the Future of Intelligence (CFI) and Language Technology Lab (LTL) seminar on Human-Centric AI Technologies\\nWord embeddings are typically treated as vectors to allow for various geometric operations. We question their very nature and whether they are vectors at all. We propose an alternative, statistical perspective on these objects, leading to state-of-the-art similarity measures defined from elementary correlation coefficients.","bio":""},{"type":"seminar","title":"Natural Language Understanding and Generation with Abstract Meaning Representation","speaker":"Marco Damonte","affiliation":"the University of Edinburgh","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 21 November 2019","time":" 11:00 - 12:00","abstract":"In this talk, I will discuss my recent work on parsing and generation with Abstract Meaning Representation (AMR).  AMR  is a semantic representation for natural language that represents sentences as graphs, where nodes represent concepts and edges represent semantic relations between them. Sentences are represented as graphs and not trees because nodes can have multiple incoming edges, called reentrancies. These are due to several linguistic phenomena such as control, coreference, and coordination.\\nI will present my work on  AMR  parsing (from text to  AMR ) and  AMR -to-text generation (from  AMR  to text). For the parsing task, we showed that it is possible to use techniques from tree parsing and adapt them to parse  AMR  graphs. To better analyze the quality of  AMR  parsers, we developed a set of fine-grained metrics to better assess parsers, including a metric for reentrancy prediction. Hence, we performed a study of the main causes of reentrancies in  AMR  and their impact on performance. For the generation task, we showed that neural encoders that have access to reentrancies outperform those who do not, demonstrating the importance of reentrancies also for generation.\\nI will also discuss the problem of using  AMR  for languages other than English. Annotating new  AMR  datasets for other languages is an expensive process and requires defining ad-hoc annotation guidelines for each new language. It is therefore reasonable to ask whether we can share  AMR  annotations across languages.","bio":""},{"type":"seminar","title":"BioNLP Research at Google","speaker":"Ryan McDonald","affiliation":"Google","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 14 November 2019","time":" 11:00 - 12:00","abstract":"In this talk I will highlight some ongoing efforts at Google on improving discovery from biomedical literature. Topics will include research in retrieval, evidence extraction, answer linking and network embedding for biomedical ontologies. I will focus on how each of these interact to create a cohesive tool to accelerate literature search. Most of the talk will focus on document and evidence retrieval, which is the entry point for all literature tools. I will discuss advances in deep learning in this space and show they make a noticeable impact in quality for the domain. Specifically, recent models that jointly extract relevant documents and focused evidence passages can increase retrieval metrics substantially. Joint work with many colleagues at Google Research and Athens University of Economics and Business.","bio":""},{"type":"seminar","title":"Learning region based representations of categories","speaker":"Steven Schockaert","affiliation":"Cardiff University","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 31 October 2019","time":" 11:00 - 12:00","abstract":"The use of vectors for representing the entities from a given knowledge base is now standard practice in Natural Language Processing. From a knowledge representation point of view, however, it seems more intuitive to model categories as regions (or distributions) rather than single vectors. A given individual is then assumed to belong to some category if the vector representation of that individual belongs to the corresponding region. Apart from increasing the interpretability of vector space representations, such region based representations also significantly expand the range of knowledge that can be expressed. In particular, we can show that region based vector space representations are able to capture a large sub-fragment of the class of existential rules. Unfortunately, estimating meaningful region representations in high-dimensional vector spaces is challenging, especially because they often have to be estimated from a very small number of examples. In our recent work, we have proposed a number of solutions that try to alleviate the lack of sufficient training examples by exploiting prior knowledge about the semantic relationships between different categories.","bio":""},{"type":"seminar","title":"Unsupervised Question Answering","speaker":"Patrick Lewis","affiliation":"UCL","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 24 October 2019","time":" 11:00 - 12:00","abstract":"Obtaining training data for Question Answering (QA) is time-consuming and costly, and existing QA datasets are only available for limited domains and languages. In this talk, we\u2019ll explore to what extent high quality training data is actually required for Extractive QA, and investigate the possibility of unsupervised Extractive QA. We approach this problem by first learning to generate context, question and answer triples in an unsupervised manner, which we then use to synthesize Extractive QA training data automatically.  We find that modern QA models can learn to answer human questions surprisingly well using only synthetic training data. We demonstrate that, without using the SQuAD training data at all, our approach achieves 56.4 F1 on SQuAD v1 (64.5 F1 when the answer is a Named entity mention), outperforming early supervised models.\\nWe will also explore methods to build cross-lingual Question Answering models which do not require cross-lingual supervision (zero-shot language transfer), as well as the challenge of how to fairly evaluate their performance in many target languages.","bio":""},{"type":"seminar","title":"Tensor product representations for RNNs / Revisiting post-processing for word embeddings","speaker":"Shuai Tang","affiliation":"University of California, San Diego","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 17 October 2019","time":" 11:00 - 12:00","abstract":"1/ Tensor product representations for RNNs\\nWidely used recurrent units, including Long-short Term Memory (LSTM) and the Gated Recurrent Unit (GRU), perform well on natural language tasks, but their ability to learn structured representations is still questionable. Exploiting reduced Tensor Product Representations (TPRs)\\n\u2014 distributed representations of symbolic structure in which vector-embedded symbols are bound to vector-embedded structural positions\u2014 we propose the  TPRU , a simple recurrent unit that, at each time step, explicitly executes structural-role binding and unbinding operations to incorporate structural information into learning. The gradient analysis of our proposed  TPRU  is conducted to support our model design, and its performance on multiple datasets shows the effectiveness of our design choices. Furthermore, observations on linguistically grounded study demonstrate the interpretability of our  TPRU .\\n2/ Revisiting post-processing for word embeddings\\nWord embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/Gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks.","bio":""},{"type":"seminar","title":"Latent Variable Models for Text Generation","speaker":"Xiaoyu Shen","affiliation":"Saarland University / Max Planck Institute","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 10 October 2019","time":" 11:00 - 12:00","abstract":"Latent variable models provide an effective way to specify\\nprior knowledge and uncover the intermediate decision process of natural language generation. In this talk, we will go through two specific applications. The first one incorporates latent continuous variables into a dialogue generation model. The latent variable is trained to maximize the mutual information with neighboring utterances. We show the\\nlatent variable component is able to significantly enhance the connection between the generated response and its surrounding context, leading to a more engaging human-machine conversation. The second one explicitly models the content selection process with discrete latent variables. By lowering down the training variance with a variational autoencoder objective, the model is able to successfully decouple content selection from the black-box generation model on both sentence compression and data-to-text tasks, enabling us to control the content selection in an interpretable way.","bio":""},{"type":"seminar","title":"Relational knowledge in vector spaces","speaker":"Luis Espinosa-Anke (University of Cardiff).","affiliation":"","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 06 June 2019","time":" 11:00 - 12:00","abstract":"In this talk a number of unsupervised approaches for learning vectors that capture relational information will be described. The main motivation behind this is that the amount of information that can be encoded in a word embedding is limited, and constrained by the similarity structure imposed by the typical methods based on co-occurrence statistics. For example, the relations holding between lion and zebra, movie theater and popcorn or dog and porch are all seemingly intuitive for us. But it is reasonable to assume that an explicit encoding capturing the subtle nature of these relations would be more appropriate than \u201csimply\u201d manipulating their word vectors. While such encodings may be acquired from external resources (e.g., knowledge bases like ConceptNet or lexical taxonomies like WordNet), these would be inherently limited, among others, by their symbolic nature. Finally, in addition to methods for learning relational knowledge, experimental results will be discussed, showing their benefit in lexical semantics tasks, text classification, and for modeling collocations.","bio":""},{"type":"seminar","title":"Imitation learning, zero-shot learning and automated fact checking","speaker":"Andreas Vlachos (University of Cambridge).","affiliation":"","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 30 May 2019","time":" 11:00 - 12:00","abstract":"In this talk I will give an overview of my research in machine learning for natural language processing. I will begin by introducing my work on imitation learning, a machine learning paradigm I have used to develop novel algorithms for structure prediction that have been applied successfully to a number of tasks such as semantic parsing, natural language generation and information extraction. Key advantages are the ability to handle large output search spaces and to learn with non-decomposable loss functions. Following this, I will discuss my work on zero-shot learning using neural networks, which enabled us to learn models that can predict labels for which no data was observed during training. I will conclude with my work on automated fact-checking, a challenge we proposed in order to stimulate progress in machine learning, natural language processing and, more broadly, artificial intelligence.","bio":""},{"type":"seminar","title":"Predicting and Analysing Online User Behaviour with Natural Language Processing","speaker":"Nikolaos Aletras (University of Sheffield).","affiliation":"","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 23 May 2019","time":" 11:00 - 12:00","abstract":"Studying the behaviour of social media users is an important problem in computational social science. Automated inference of user characteristics has applications in personalised recommender systems, targeted computational advertising and online political campaigning. In this talk, I will present three case studies: (1) inferring socioeconomic characteristics (e.g., income and occupational class); (2) studying voting behaviour; and (3) complaints.","bio":""},{"type":"seminar","title":"How to Pay Attention: Learning to Transfer Knowledge between Sentences and Tokens","speaker":"Marek Rei ( University of Cambridge).","affiliation":"","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 09 May 2019","time":" 11:00 - 12:00","abstract":"Self-attention architectures allow models to dynamically decide which areas of the input should receive more focus. During the construction of text representations, attention weights also provide a way of quantifying the importance of different input areas. In this talk, we investigate how attention mechanisms can be turned into sequence labelers, opening up some new and interesting applications. These networks learn to predict labels for individual tokens, based only on sentence-level supervision, even without having seen any examples of sequence labeling. In addition, optimizing on the token level explicitly teaches the model where it should be focusing, leading to improvements in text classification. We will also discuss experiments with learning directly from the human cognitive signal, guiding the models to internally behave more like their users. The resulting architectures for text classification and sequence labeling are more accurate, more interpretable and make decisions in more predictable ways.","bio":""},{"type":"seminar","title":"Finding translations in unordered text using multilingual sentence representations","speaker":"Jenna Kanerva","affiliation":"University of Turku","link":"","venue":"offline","place":"Board room, Faculty of English, 9 West Rd (Sidgwick Site)","date":"Thursday 04 April 2019","time":" 11:00 - 12:00","abstract":"Machine translation (e.g. Finnish-Swedish) can be improved by collecting all possible translated material available. One approach is to find translated sentences from large collections of unordered, monolingual data (e.g. monolingual web crawls) by embedding all sentences with a multilingual encoder and searching for most similar Finnish-Swedish \\nsentence pairs.","bio":""},{"type":"seminar","title":"Disagreements in anaphoric interpretation","speaker":"Massimo Poesio ( University of Essex).","affiliation":"","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 14 March 2019","time":" 11:00 - 12:00","abstract":"The assumption that natural language expressions have a single, discrete and clearly identifiable meaning in a given context, successfully challenged in lexical semantics by the rise of distributional models, nevertheless still underlies much work in computational linguistics, including work based on distributed representations. In this talk I will first of all present the evidence that convinced us that the assumption that a single interpretation can always be assigned to anaphoric expression is no more than a convenient idealization. I will then discuss recent work on the  DALI \\nproject that aims to develop a new model of interpretation that abandons this assumption for the case of anaphoric interpretaton / coreference. I will present the recently released Phrase Detectives 2.1 corpus, containing around 2 million crowdsourced judgments for more than 100,000 markables, an average of 20 judgments per markable;\\nthe Mention Pair Annotation (MPA) Bayesian inference model developed to aggregate these judgments; and the results of a preliminary analysis of disagreements in the corpus suggesting that between 10% and 30% of markables in the corpus appear to be genuinely ambiguous.\\nJoint work with Jon Chamberlain, Silviu Paun, Alexandra Uma, \\nJuntao Yu, Richard Bartle and Udo Kruschwitz","bio":""},{"type":"seminar","title":"Improving Literature-based Discovery with Neural Networks","speaker":"Gamal Crichton","affiliation":"Language Technology Lab","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 07 March 2019","time":" 11:00 - 12:00","abstract":"Literature-based Discovery (LBD) uses information from explicit statements in literature to generate new knowledge and can thus facilitate hypothesis testing and generation from publications to accelerate scientific research. Existing methods, however, use methodologies which are inadequate for capturing the complex information available in scientific literature and are prone to proposing spurious or low-quality discoveries. Recent advances in  NLP  allow for deep textual analysis to obtain a wide coverage of information in text and \\nadapt to recognising new entities. Similarly, recent advances in graph processing have made it possible to do in-depth analysis on information represented as graphs to facilitate knowledge discovery. Both advances utilise neural networks extensively. This work used neural networks to advance  LBD  by: improving biomedical  NER  using multi-task learning; \\nimproving knowledge discovery from biomedical graphs using link prediction; and improving the ranking of published discoveries by scoring the strength of connection paths. Excitingly, the latter approaches outperformed those used by the state-of-the-art  LION LBD  tool. These results show that it is feasible to use neural networks to improve this increasingly necessary task and that neural biomedical \\nknowledge discovery is potent, operational and a potentially rich field for further study.","bio":""},{"type":"seminar","title":"Teaching Artificial Agents to Understand Language by Modelling Reward","speaker":"Edward Grefenstette","affiliation":"Facebook AI Research","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 21 February 2019","time":" 11:00 - 12:00","abstract":"Abstract: Recent progress in Deep Reinforcement Learning has shown that agents can be taught complex behaviour and solve difficult tasks, such as playing video games from pixel observations, or mastering the game of Go without observing human games, with relatively little prior information. Building on these successes, researchers such as Hermann and colleagues have sought to apply these methods to teach\u2013in simulation\u2013agents to complete a variety of tasks specified by combinatorially rich instruction languages. In this talk, we discuss some of these highlights and some of the limitations which inhibit scalability of such approaches to more complex instruction languages (including natural language). Following this, we introduce a new approach, inspired by recent work in adversarial reward modelling, which constitutes a first step towards scaling instruction-conditional agent training to \u201creal world\u201d language.","bio":"Edward Grefenstette is a Research Scientist at Facebook  AI  Research, and Honorary Associate Professor at  UCL . Prior to this, he was a Staff Research Scientist at DeepMind. He completed his DPhil (PhD) at the University of Oxford in 2013 under the supervision of Profs Coecke and Pulman, and Dr Sadrzadeh, working on applying category-theoretic tools\u2013initially developed to model quantum information flow\u2013to model compositionality of distributed representations in natural language semantics. His recent research has covered topics at the intersection of deep learning and machine reasoning, addressing questions such as how neural networks can model or understand logic and mathematics, infer implicit or human-readable programs, or learn to understand instructions from simulation."},{"type":"seminar","title":"Do Deep Generative Models Know What They Don\'t Know?","speaker":"Eric T Nalisnick (University of Cambridge).","affiliation":"","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 14 February 2019","time":" 11:00 - 12:00","abstract":"Abstract:\\nA neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to \\nsuch overconfident mistakes as modeling the density of the input features can be used to detect novel, out-of-distribution inputs.  In this talk, I challenge this assumption, focusing analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find that the model density cannot distinguish images of common objects such as dogs, trucks, and horses (i.e.  CIFAR -10) from those of house numbers (i.e.  SVHN ), assigning a higher likelihood to the latter when the model is trained on the former. We find such behavior persists even when we restrict the flows to \\nconstant-volume transformations. These admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results suggest caution when using density estimates of deep generative models on out-of-distribution inputs.","bio":""},{"type":"seminar","title":"Probabilistic Typology","speaker":"Ryan Cotterell ( University of Cambridge).","affiliation":"","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 07 February 2019","time":" 11:00 - 12:00","abstract":"TBA","bio":""},{"type":"seminar","title":"Learning and testing compositionality","speaker":"Elia Bruni","affiliation":"University of Pompeu Fabra (Barcelona, Spain)","link":"","venue":"offline","place":"Faculty of English, Room SR24","date":"Thursday 24 January 2019","time":" 11:00 - 12:00","abstract":"While sequence-to-sequence (seq2seq) models have shown remarkable generalisation power across several natural language tasks, their construct of solutions is argued to be less compositional than human-like generalisation. In this talk, I will discuss our attempts to narrow this gap.\\nIn the first part of the talk, I will introduce the notion of compositionality and explain why it is crucial for artificial learners to master it if they want to talk and think like people. I will then present three different strategies we followed to bias seq2seq models towards more compositional solutions.\\nFirst, I will talk about Attentive Guidance (AG), a new mechanism to direct a seq2seq model equipped with attention to find more compositional solutions. Models trained with AG come up with solutions that, in some cases, fit the training and testing distributions\\nequally well.\\nWhile AG is effective, it has the problem that needs an extra supervision signal. As a remedy, I will present sequence-to-attention, a new architecture that we specifically designed to exploit attention to find compositional patterns in the input without the need of extra-supervision. The solutions found by the model are highly interpretable, allowing easy analysis of both the types of solutions that are found and potential causes for mistakes.\\nLastly, I will present some preliminary results of a second architecture where we are trying to disentangle content- and position-based representations in the attention mechanism of a seq2seq. This again helps with interpretability but also with extrapolating to longer sequences than the ones seen by the model during training. Furthermore, it gives us the chance to design a more human-like version of (positional) attention.\\nIn the second part of the talk, I will argue that currently,\\nit is difficult to test for compositionality in neural networks. I will then present our compositional manifesto, a new battery of tests to assess the compositional abilities in seq2seq models. In particular, I will introduce five tests: Localism, Substitutivity, Productivity, Systematicity and Overgeneralisation. I will then \u201ctest the tests\u201d using three instances of a seq2seq model: a recurrent-seq2seq, a convolutional-seq2seq, and a Transformer network.\\nTo conclude, I will highlight new research directions relating to compositional learning where I aim to ground the learners in the visual world.","bio":""},{"type":"seminar","title":"Linguistic Measures for the Detection of Clinical Conditions","speaker":"Aline Villavicencio (Federal University of Rio Grande do Sul","affiliation":"University of Essex)","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 29 November 2018","time":" 11:00 - 12:00","abstract":"Abstract: Tests like Semantic Verbal Fluency have been used to characterise language in typical and in clinical conditions, like Dementia. For instance, given a sequence of semantically related words, a large number of switches from one semantic class to another has been linked to clinical conditions. In this talk I discuss work on using semantic similarity measures for characterising groups of speakers and how some measures can be used to inform the detection of clinical conditions. In one study this information is used for building classifiers to distinguish healthy controls from clinical cases with early stages of Alzheimer\u2019s Disease and Mild Cognitive Deficits. The results obtained indicate that the classifiers that use these similarity measures outperform those that use a gold standard taxonomy.\\nShort","bio":"Aline Villavicencio is a Reader in Computer Science affiliated to the Federal University of Rio Grande do Sul (Brazil) and also affiliated to the University of Essex (UK). Her research interests include lexical semantics, multilinguality, and cognitively motivated  NLP . She received her PhD from the University of Cambridge (UK) in 2001, and held postdoc positions at the University of Cambridge and University of Essex (UK). During 2011-2012 and 2014-2015, she was on sabbatical at the Massachusetts Institute of Technology (USA). She is a current member of the editorial board of the Journal of Natural Language Engineering, the Transactions of the Association for Computational Linguistics, among others, and was Area Chair for  NAACL 2018 , for  COLING 2018  and for  IBERAMIA 2018 , and the Chair for the International Conference on Computational Processing of Portuguese (PROPOR 2018). She is also a regular member of the program committee for the various  ACL  conferences, and has co-chaired numerous *ACL workshops on Cognitive Aspects of Computational Language Acquisition and on Multiword Expressions. She has co-edited special issues and books dedicated to these topics."},{"type":"seminar","title":"Emergence of (linguistic) communication through multi-agent interactions","speaker":"Angeliki Lazaridou","affiliation":"DeepMind","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 22 November 2018","time":" 11:00 - 12:00","abstract":"Abstract: Distributional models and other supervised models of language focus on the structure of language and are an excellent way to learn general statistical associations between sequences of symbols. However, they do not capture the functional aspects of communication, i.e., that humans have intentions and use words to coordinate with others and make things happen in the real world. In this talk, I will present two studies on multi-agent emergent communication, where agents exist in some grounded environment and have to communicate about objects and their properties. This process requires the negotiation of linguistic meaning in this pragmatic context of achieving their goal. In the first study, I will present experiments in which agents learn to form a common ground that allow them to communicate about disentangled (i.e., feature norm) and entangled (i.e., raw pixels) input. In the second study, I will talk about properties of linguistic communication as arising in the context of self-interested agents.","bio":""},{"type":"seminar","title":"Learning with Explanations","speaker":"Tim Rockt\xe4schel","affiliation":"Facebook AI Research","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 15 November 2018","time":" 11:00 - 12:00","abstract":"Abstract: Despite the success of deep learning models in a wide range of applications, these methods suffer from low sample efficiency and opaqueness. Low sample  efficiency limits the application of deep learning to domains for which abundant training data exists whereas\\nopaqueness prevents us from understanding how a model derived a particular output, let alone how to correct systematic errors, how to remove bias, or how to incorporate common sense and domain knowledge. To address these issues for knowledge base completion, we developed end-to-end differentiable provers which (i) learn neural representations of symbols in a knowledge base, (ii) make use of similarities between learned symbol representations to prove queries to the knowledge base, (iii) induce logical rules, and (iv) use provided and induced rules for multi-hop reasoning. I will present our recent efforts in applying differentiable provers to statements in natural language texts and large-scale knowledge bases. Furthermore, I will introduce two datasets for advancing the development of models capable of incorporating natural language explanations: eSNLI, crowdsourced explanations for over half a million sentence pairs in the Stanford Natural Language Inference corpus, and ShARC, a conversational question answering dataset with natural language rules.","bio":""},{"type":"seminar","title":"Cross-Lingual Word Embeddings in 60 Minutes","speaker":"Ivan Vuli\u0107","affiliation":"LTL, University of Cambridge","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 08 November 2018","time":" 11:00 - 12:00","abstract":"Abstract: In recent past,  NLP  as a field has seen tremendous utility of word embeddings as features in downstream tasks. The fact that these word vectors can be trained on unlabeled monolingual corpora of a language makes them an inexpensive resource in  NLP . With the increasing use of monolingual word vectors, there is a need for word vectors that can be used as efficiently across multiple languages as monolingually. Therefore, learning bilingual and multilingual word embeddings is currently an important research topic. These vectors offer an elegant and language-pair independent way to represent content across different languages in shared cross-lingual embedding spaces, and also enable the integration of knowledge from external resources (e.g., WordNet, dictionaries) into the embedding spaces. In this mini-tutorial, I will briefly discuss the current techniques in cross-lingual word embedding learning, presenting the model typology based on multilingual training data requirements, also including very recent zero-supervision methods that require no bilingual data at all. I will then introduce several illustrative applications of the induced embedding spaces, including bilingual dictionary induction, ad-hoc cross-lingual information retrieval, and cross-lingual transfer for dependency parsing and dialogue state tracking.","bio":""},{"type":"seminar","title":"On KL divergence and beyond","speaker":"Yingzhen Li","affiliation":"Microsoft Research Cambridge","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 01 November 2018","time":" 11:00 - 12:00","abstract":"Abstract: Many machine learning tasks require fitting a model to observed data, which is mostly done via divergence minimisation. In this talk I will start from the basics and discuss the celebrated Kullback-Leibler (KL) divergence and its applications in machine learning. Then I will discuss potential issues of KL divergence and motivates other divergence measures. I will show how f-divergence, a rich family of divergences that includes KL, is applied to machine learning tasks, in particular for approximate inference. If time permits, I will briefly touch on divergencies/discrepancies that are not density-ratio based, and discuss relavent applications.","bio":""},{"type":"seminar","title":"Representation Learning and Neuro-Symbolic Reasoning in Knowledge Graphs and Natural Language","speaker":"Pasquale Minervini","affiliation":"University College London","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 25 October 2018","time":" 11:00 - 12:00","abstract":"Abstract: Knowledge Graphs (KGs) are graph-structured Knowledge Bases where facts are encoded by links between entities, and their use is pervasive both in industry (e.g. see Google Knowledge Graph, Microsoft Satori) and academia (YAGO, DBpedia,  NELL , OpenCyc). KGs are extremely useful, as they provide automated systems with machine-readable domain specific knowledge, allowing AI systems to perform deductive (and inductive) reasoning in a variety of domains. The talk will discuss about Representation Learning in KGs, its shortcomings, how we can introduce First-Order Logic constraints in the learned representations, and how we can extend such techniques to  NLP  models. Then the speaker will talk about his recent work in neuro-symbolic reasoning \u2013 namely on Neural Theorem Provers [1, 2] \u2013 and how we can perform explainable deductive reasoning jointly on Knowledge Graphs and Natural Language at scale.\\n[1] https://arxiv.org/abs/1705.11040\\n[2] https://arxiv.org/abs/1807.08204","bio":"Pasquale Minervini is a Postdoctoral Research Associate in Statistical Natural Language Processing\\nand Machine Learning in the  UCL  Machine Reading group. He is funded by a Machine Reading grant from the Allen Institute\\nfor Artificial Intelligence (AI2)."},{"type":"seminar","title":"Practical Approaches to Conversational AI","speaker":"Pei-Hao (Eddy) Su","affiliation":"PolyAI","link":"","venue":"offline","place":"Board Room, 1st floor, English Faculty Building (Sidgwick site), 9 West Road, Cambridge","date":"Thursday 18 October 2018","time":" 11:00 - 12:00","abstract":"Abstract: In this talk, I will describe the status of the dialogue research in the  NLP  community by providing the research background, a survey of available resources, and giving key insights to the application of state-of-the-art dialogue methodology into industry-scale systems. Despite the phenomenal potential of deep learning methods in this sector, very few of these approaches are production-ready. I will walk through these approaches and summarise their pros and cons.","bio":"Pei-Hao (Eddy) Su is a co-founder and Chief Scientist of PolyAI, a London-based startup developing the next generation machine learning platform for building conversational user interfaces. Eddy completed his PhD at the University of Cambridge, working with Professor Steve Young. His research interests centre on applying deep learning, reinforcement learning and Bayesian approaches to dialogue management and reward estimation. He received the best student paper award at  ACL 2016  on his dialogue work and has given a tutorial on deep learning for conversational AI at  NAACL 2018 . He was a research intern at Facebook  AI  Research in Summer 2017."},{"type":"seminar","title":"LION-LBD: Literature-Based Discovery for Cancer Biology","speaker":"Simon Baker","affiliation":"LTL","link":"","venue":"offline","place":"Board Room, 1st floor, English Faculty Building (Sidgwick site), 9 West Road, Cambridge","date":"Thursday 11 October 2018","time":" 11:00 - 12:00","abstract":"Abstract: The overwhelming size and rapid growth of the biomedical literature make it impossible for scientists to read all studies related to their work, potentially leading to missed connections and wasted time and resources.\\nWe have developed  LION -LBD, a literature-based discovery system that helps cancer researchers to make new discoveries from already published text. The system supports the idea of hypothesis generation and testing with the aid of text mining. The system is built with a particular focus on the molecular biology of cancer using state-of-the-art natural language processing, including named entity recognition and grounding to domain ontologies covering a wide range of entity types and a novel approach to detecting references to the hallmarks of cancer in text.","bio":"Simon Baker is a research associate (postdoc) at the Language Technology Lab (LTL). He also collaborates with the Natural Language and Information Processing (NLIP) group at the Computer Laboratory. His current research interests include: information extraction, text mining, and related applications such as automatic Literature-based Discovery (LBD). He works largely in the biomedical domain."},{"type":"seminar","title":"Language Adaptation experiments: Cross-lingual embeddings for related languages","speaker":"Serge Sharoff (Leeds).","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 14 June 2018","time":" 11:00 - 12:00","abstract":"We do not have sufficient annotated resources for many languages. For example, out of the 47 languages in the Universal Depencies set only two languages have treebanks exceeding 1 mln words, while many languages are not represented at all.  However, we can use typological links between the languages to produce better computational models for lesser-resourced languages (recipients) by transferring information from better resourced ones (donors).  In this talk I report my experience of building and using cross-lingual embeddings with added constraints such as the Weighted Levenshtein Distance.","bio":""},{"type":"seminar","title":"Methods and Interactive Tools for Exploring the Semantics of Essentially Contested Concepts","speaker":"Paul Nulty (CRASSH-Cambridge).","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 07 June 2018","time":" 11:00 - 12:00","abstract":"Political concepts are often characterised as \u201cessentially contested\u201d in the sense that their essential meanings are necessarily in dispute when we deploy them in adversarial political discourse. When tasked with precisely describe the meaning of an abstract word, modern lexicographers and computational linguists turn to a descriptive analysis of the term\u2019s use in context, looking for statistical patterns of syntactic or window-based word co-occurrence in large collections of digital text. This talk presents an application of the tools of distributional semantics to the problem of delineating the various complex, multi-faceted, and value-laden meanings of abstract political concepts. I will demonstrate interactive applications developed to allow researchers in intellectual history to explore these conceptual structures in historical corpora. This work is carried out as part of The Concept Lab, an interdisciplinary research group based at  CRASSH , Cambridge.","bio":""},{"type":"seminar","title":"Imitation learning for language generation","speaker":"Gerasimos Lampouras (Sheffield).","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 31 May 2018","time":" 11:00 - 12:00","abstract":"Natural language generation (NLG) is the task of generating natural language from a meaning representation. Rule-based approaches require domain-specific and manually constructed linguistic resources, while most corpus based approaches rely on aligned training data and/or phrase templates. The latter are needed to restrict the search space for the structured prediction task defined by the unaligned datasets.  In this talk we will discuss the use of imitation learning for structured prediction which learns an incremental model that handles the large search space while avoiding explicitly enumerating it. We will show how we adapted the Locally Optimal Learning to Search (Chang et al., 2015) framework which allows us to train against non-decomposable loss functions such as the  BLEU  or  ROUGE  scores while not assuming gold standard alignments. Furthermore, we will present an analysis of the datasets which examines common issues with  NLG  evaluation.","bio":""},{"type":"seminar","title":"Scalable Non-Markovian Language Modelling","speaker":"Ehsan Shareghi.","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 03 May 2018","time":" 11:00 - 12:00","abstract":"Markov models are popular means of modeling the underlying structure of natural language, which is naturally represented as sequences and trees. The locality assumption made in low-order Markov models such as n-gram language models is limiting, because if the data generation process exhibits long range dependencies, modeling the distribution well requires consideration of long range context. On the other hand, higher-order Markov, or infinite-order Non-Markovian (infinite-order Markov) models, exhibit computational complexity and statistical challenges during learning and inference. In particular, under the large data setting their exponential number of parameters often results in estimation and sampler mixing issues, while representing the structure of the model, and sufficient statistics or sampler states can quickly become computationally inefficient and impractical.\\nWe propose a framework based on compressed data structures which keeps the memory usage of modeling, learning, and inference steps independent from the order of the models. Our approach scales nicely with the order of the Markov model and data size, and is highly competitive with the state-of-the-art in terms of the memory and runtime, while allowing us to develop Bayesian and non-Bayesian smoothing techniques. Using our compressed framework to represent the models, we explore its scalability under two Non-Markovian language modeling settings, using large scale data and infinite context.\\nFirst, we model the Kneser-Ney family of language models and illustrate that our approach is several orders of magnitude more memory efficient than the state-of-the-art, in training and testing, while it is highly competitive in terms of run-times of both phases. When memory is a limiting factor at query time, our approach is orders of magnitude faster than the state-of-the-art.  We then turn to Hierarchical Nonparametric Bayesian language modeling, and develop efficient sampling mechanism which allows us to prevent the sampler mixing issue, common in large Bayesian models. More precisely, compared with the previous stat-of-the-art hierarchical Bayesian language model, the experimental  results  illustrate  that  our  model  can be  built  on  100x  larger  datasets, while being several orders  of  magnitude  smaller, fast  for  training  and inference, and outperforming the perplexity of the state-of-the-art  Modified  Kneser-Ney LM by up to 15%.","bio":""},{"type":"seminar","title":"Knowledge Acquisition from Corpora and the Wisdom of Crowds","speaker":"Daisuke Kawahara (Kyoto University).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 22 March 2018","time":" 11:00 - 12:00","abstract":"To realize true artificial intelligence, it is essential to make computers understand the meaning of text. A bottleneck in text understanding has been the lack of commonsense knowledge. However, in recent years, this situation has been changing by automatic knowledge acquisition from the Web and the wisdom of crowds. In this talk, I will introduce our recent approaches to knowledge acquisition mainly for Japanese and text analysis methods based on the acquired knowledge.","bio":""},{"type":"seminar","title":"NLP in the clinical domain - data, approaches and considerations","speaker":"Sumithra Velupillai (King\'s College).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 01 March 2018","time":" 11:00 - 12:00","abstract":"Clinical  NLP  is the application of text processing approaches on documents written by healthcare professionals in clinical settings, such as notes and reports in health records. The interest for clinical  NLP  is spurred by the need for real-time, largescale, and accurate information extraction from health records to support clinical care, e.g., through automated generation of a patient problem list, to support biomedical and health services research, e.g., through precise cohort identification, and to support public health practice, e.g., through disease surveillance. Clinical  NLP  can provide clinicians with critical patient case details, which are often locked within unstructured clinical texts and dispersed throughout a patient\u2019s health record. Recently, patient-generated text such as posts on social media forums related to health have also received increased interest for population-based studies.\\nIn this talk, I will describe work on automated extraction of clinically relevant information from clinical text, including semantic aspects such as negation, uncertainty and time information. I will give some examples from my experience in working with Swedish and English health record data, collaborating with clinicians and considerations needed when working with this type of data, as well as my recent experiences with working in the mental health domain, including studies on social media data.","bio":""},{"type":"seminar","title":"Predicting Judicial Decisions of the European Court of Human Rights","speaker":"Nikos Aletras (Sheffield).","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 15 February 2018","time":" 11:00 - 12:00","abstract":"Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e., N-grams, and topics. Our models can predict the court\u2019s decisions with a strong accuracy (79% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.","bio":""},{"type":"seminar","title":"Graph Convolutional Networks for Natural Language Processing and Relational Modeling","speaker":"Ivan Titov (Edinburgh).","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 08 February 2018","time":" 11:00 - 12:00","abstract":"Graph Convolutional Networks (GCNs) is an effective tool for modeling graph structured data. We investigate their applicability in the context of natural language processing (machine translation and semantic role labelling) and modeling relational data (link prediction). For natural language processing,  we introduce a version of GCNs suited to modeling syntactic and/or semantic dependency graphs and use them to construct linguistically-informed sentence encoders. We demonstrate that using them results in a substantial boost in machine translation performance and state-of-the-art results on semantic role labeling of English and Chinese. We also experiment with GCNs over latent graphs (i.e. use structure of a sentence as a latent variable). For link prediction, we propose Relational GCNs (RGCNs), GCNs developed specifically to deal with highly multi-relational data, characteristic of realistic knowledge bases. By explicitly modeling neighbourhoods of entities,  RGC Ns accumulate evidence over multiple inference steps in relational graphs and yield competitive results on standard link prediction benchmarks.\\nJoint work with Diego Marcheggiani, Michael Schlichtkrull, Joost Bastings, Thomas Kipf, Wilker Aziz, Max Welling, Khalil Sima\u2019an, Rianna van den Berg and Peter Bloem.","bio":""},{"type":"seminar","title":"Interdisciplinarity: The art of unsettling multiple disciplines","speaker":"Petar Milin (Sheffield).","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 25 January 2018","time":" 11:00 - 12:00","abstract":"Charles Darwin once said that \u201cit is the long history of humankind (and animal kind, too) that those who learned to collaborate and improvise most effectively have prevailed\u201d. On first encounter, many may find interdisciplinarity unsettling as it appears to devalue disciplinary expertise and the identity that comes with such expertise. And maybe it really does: integrative interdisciplinarity relies on mutually complementary theories, shared testable hypotheses, and interspersed methodological endeavour. Anything that is shared across disciplines will, by definition, not coincide with anyone\u2019s customary way of doing things.\\nIn my talk I will present two case studies to illustrate the rationale behind the Leverhulme- funded research project \u201cOut of Our Minds\u201d. The overarching aim of this project is to propose a novel way of describing language data that yields a cognitively plausible description of speakers\u2019 linguistic knowledge. The research methodology combines corpus-based analyses, behavioural experimentation and computational modelling, thereby embodying the true interdisciplinary core of what we like to call Language Sciences.\\nThe first study scrutinizes the role orthographic and semantic information play in the behaviour of skilled readers. Reading latencies from a self-paced sentence reading experiment in which Russian near-synonymous verbs were manipulated appear well- predicted by a complex interplay of bottom-up and top-down support from orthography and semantics. Individual differences in mental speed modulate this interplay and show a fascinating complexity at the interface of language and behaviour.\\nThe second study sheds new light on seemingly unmotivated allomorphy in the genitive singular of Polish masculine nouns and demonstrates how biologically inspired machine learning techniques can pinpoint the essence of native speaker intuitions. The model explains the unexpected preference of -a as genitive ending for new words in terms of the learnability of words taking that ending, their phonological predictability and their contextual typicality.\\nOn their own linguists and psychologists would have approached these questions rather differently, and would have arrived at answers that would necessarily have remained partial. I hope that the prospects for further investigation that this interdisciplinary approach opens up will convince linguists, psychologists and computer scientists to bury the hatchet, relic of old disciplines bound by (parochial) traditions, and to set off on a joint journey towards a unifying perspective on language phenomena.","bio":""},{"type":"seminar","title":"Context Sensitive Distributional Semantics","speaker":"Stephen McGregor","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 18 January 2018","time":" 11:00 - 12:00","abstract":"In this talk I\u2019ll discuss a novel approach to the established distributional semantic paradigm for modelling words as vectors in semantically productive spaces.  Motivated by theoretical insight into the way that word meaning emerges situationally, this methodology seeks to generate ad hoc lexical representations based on context specific input.  I will describe a technique for projecting lower dimensional semantic subspaces based on the idea that semantic contexts can be construed as perspectives on a very high dimensional space of co-occurrence statistics.  The expectation is that the geometry of these subspaces will correspond to properties that are specific to the conceptual context with which they are associated.  I\u2019ll demonstrate how this methodology can be effectively applied to  NLP  tasks such as analogy completion and word similarity ranking, and then will consider how the methodology might motivate further exploration of various features of distributional semantic models.","bio":""},{"type":"seminar","title":"Senses can help vector space models of lexical substitution","speaker":"Marianna Apidianaki (LIMSI).","affiliation":"","link":"","venue":"offline","place":"Boardroom, Faculty of English, West Road","date":"Thursday 11 January 2018","time":" 11:00 - 12:00","abstract":"The role of senses in  NLP  applications has been questioned due to the high performance of vector space models in semantic tasks. These models deliver state-of-the-art performance without explicitly accounting for senses which have even been shown to be harmful for some tasks. In this talk, I will show how sense representations tailored to the task can improve the results of vector-based lexical substitution models. I will discuss two aspects related to paraphrase substitution, namely their clusterability into senses and their substitutability in context. Finally, I will present preliminary results on core sense detection through a multi-view approach to paraphrase semantic analysis.","bio":""},{"type":"seminar","title":"Sentence Generation using a Dynamic Canvas","speaker":"David Barber (University College London).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 30 November 2017","time":" 11:00 - 12:00","abstract":"I will discuss the Attentive Unsupervised Text Writer (AUTR), a word level generative model for natural language. It uses a recurrent neural network with a dynamic attention and canvas memory mechanism to iteratively construct sentences. By viewing the state of the memory at intermediate stages and where the model is placing its attention, we gain insight into how it constructs sentences. We demonstrate that  AUTR  learns a useful latent representation for each sentence and achieves competitive log-likelihood lower bounds whilst being computationally efficient. It is effective at generating and reconstructing sentences, as well as imputing missing words.","bio":""},{"type":"seminar","title":"Learning to Create and Reuse Words in Open-Vocabulary Language Modeling","speaker":"Kazuya Kawakami (DeepMind).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 23 November 2017","time":" 11:00 - 12:00","abstract":"Fixed-vocabulary language models fail to account for one of the most\\ncharacteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the \u201cbursty\u201d distribution of such words. In this talk, we discuss a hierarchical  LSTM  language model that generates sequences of word\\ntokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus;  MWC ) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.","bio":""},{"type":"seminar","title":"What can online collaborative dictionaries tell us about language and social dynamics?","speaker":"Barbara McGillivray (The Alan Turing Institute/University of Cambridge).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 09 November 2017","time":" 11:00 - 12:00","abstract":"Traditional dictionaries aim at including neologisms and new word senses, and are regularly updated, but they require enough textual evidence for inclusion and are often slow at reacting to linguistic innovation. On the other hand, online crowd-sourced dictionaries like Urban Dictionary (UD) and Wiktionary are unique sources of data for studying this type of language change as it happens; they are constantly updated and the threshold for including new material in them is lower than for traditional dictionaries. However, this comes with additional challenges. UD tends to record ephemeral quotidian spoken language and to represent popular views of meaning to a greater extent than other dictionaries. Slang and offensive language are over-represented in UD and its content often reflects the concerns of a specific community rather than wide linguistic trends. In this talk I will report on preliminary results of a study conducted in collaboration with Dr Dong Nguyen and Dr Taha Yasseri where we analysed UD data crawled in 2016 to answer the following questions:\\n- What is the distribution of entries in UD and how do they compare with Wiktionary? \\n- To what extend can UD be a reliable source for recording lexical-semantic change in the\\nEnglish language?","bio":""},{"type":"seminar","title":"Corpus-Based Analysis of the Canonical Word Order of Japanese Double Object Constructions","speaker":"Ryohei Sasano (Nagoya University).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 02 November 2017","time":" 11:00 - 12:00","abstract":"Several studies have investigated the canonical word order of Japanese double object constructions. However, most of these studies use either manual analyses or measurements of human characteristics such as brain activities or reading times for each example. Thus, although these analyses are reliable for the examples they focus on, the findings cannot be generalized to other examples. In contrast, the trend of actual usage can be automatically collected from a large corpus. In this talk, I will present a corpus-based analysis of the canonical word order of Japanese double object constructions with the assumption that there is a relation between the canonical word order and the proportion of each word order in a large corpus.","bio":""},{"type":"seminar","title":"Detecting Semantic Change Using LDA in Historical Texts: a Case Study on Dutch","speaker":"Simon Hengchen (Universit\xe9 libre de Bruxelles).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Tuesday 10 October 2017","time":" 13:00 - 14:00","abstract":"Semantic change detection is relevant to many, including historians who want to better understand their sources, or lexicographers who wish to compile dictionaries. While the traditional way of detecting semantic change is to \u201cread a lot\u201d (Cavallin 2012), the availability of large diachronic corpora in digital form and computing power allow for a more automatic and efficient way to tackle this task. This talk is in two parts: first, an  LDA -based method to detect semantic change in historical, dirty text will be presented, and then a case study will illustrate the approach. \\nIn our case study, we demonstrate a language-agnostic method on a corpus of badly-OCRed Belgian socialist newspapers in Dutch from the 19th and 20th centuries. This case study thus hints at the reproducibility of the method on other, less-resourced languages.\\nCavallin, K. (2012). Automatic extraction of potential examples of semantic change using lexical sets. In KONVENS, pages 370\u2013377","bio":""},{"type":"seminar","title":"Detecting Text Reuse in Large Historical Corpora and Authorship Attribution of Premodern Documents","speaker":"Aleksi Vesanto (University of Turku).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Tuesday 03 October 2017","time":" 11:00 - 12:00","abstract":"This presentation covers two projects: a method to detect text reuse that can withstand extreme  OCR  noise, and real world applications of machine learning in authorship attribution. Detecting text reuse from historical documents is relevant to many, as it can shed light on many questions, such as how certain news spread or whether authors have plagiarized others. Finding these repeated passages can be fairly hard, as the documents are generally  OCR  transcribed and can contain extreme noise, where the text is bordering on unreadable. Authorship attribution is in no way a new field, yet machine learning has only had a limited spotlight in real world applications. This presentation highlights a case where machine learning provides new information that contradicts older manual attributions, and a method to attribute a document with multiple possible authors with very little training data.","bio":""},{"type":"seminar","title":"Natural Language Processing to Bridge Heterogeneous Data","speaker":"Yusuke Miyao (National Institute of Informatics","affiliation":"Japan)","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Tuesday 19 September 2017","time":" 14:00 - 15:00","abstract":"Natural language is a primary means to communicate information, while\\nour daily life also relies on other types of media such as vision and\\nnumerical data.  I will talk about our recent efforts on developing\\nmethods for accessing non-text data via natural language.\\nSpecifically, i) semantic parsing for querying structured databases,\\nii) text generation for videos and time-series data, and iii) semantic\\nrepresentations for visual information, are introduced.","bio":""},{"type":"seminar","title":"Modeling Context-sensitive Selectional Preference with Distributed Representations","speaker":"Naoya Inoue (Tohoku University","affiliation":"Japan)","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 31 August 2017","time":" 11:00 - 12:00","abstract":"This talk will discuss a novel problem setting of selectional preference (SP) between a predicate and its arguments, called as context-sensitive  SP  (CSP).  CSP  models the narrative consistency between the predicate and preceding contexts of its arguments, in addition to the conventional SP based on semantic types. Furthermore, I will present a novel  CSP  model that extends a neural SP model to incorporate contextual information into the distributed representations of arguments. Experiments show that the proposed  CSP  model successfully learns  CSP  and outperforms the conventional SP model in coreference cluster ranking.","bio":""},{"type":"seminar","title":"Linguists-defined and Machine-induced Natural Language Structures to Executable Logical Forms","speaker":"Siva Reddy (Stanford University).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 15 June 2017","time":" 11:00 - 12:00","abstract":"Querying a database to retrieve an answer, telling a robot to perform an action, or teaching a computer to play a game are tasks requiring communication with machines in a language interpretable by them. Here we consider the task of converting human language to a knowledge-base (KB) language for question-answering. While human languages are sequential in nature with latent structures, machine interpretable languages have explicit formal structures. The computational linguistics community has created several treebanks to understand the formal structures of human languages, e.g., universal dependencies. But are these helpful in deriving target language structures?\\nIn the first part of the talk, I will discuss how to convert universal dependencies in multiple languages to both general-purpose and kb-executable logical forms. In the second part, I will present a neural model on how to induce task-specific natural language structures. I will discuss the similarities and differences between linguists-defined and machine-induced structures.","bio":""},{"type":"seminar","title":"Vanishing laws of semantic change - studying semantic change using distributional word representations and proper control condition","speaker":"Haim Dubossarsky (Hebrew University of Jerusalem).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Monday 12 June 2017","time":" 11:00 - 12:00","abstract":"Recent developments in  NLP  have enabled language change research to use tools that were originally developed to study synchronic similarities between words. We review and report several proposed laws of semantic change that have been found for distributional word representations based on these techniques. Furthermore, we advocate the use of control condition under which any proposed laws should be validated. We found that under a stringent control condition, many of the previously reported laws of semantic change diminished significantly,  or vanished. Specifically, word frequency is found to play an artefactual role either directly or indirectly (due to co-linearity) in many of the proposed laws of semantic change. We claim that these modified laws are in fact more credible than the original ones due to the diverse factors that are involved in semantic change.","bio":""},{"type":"seminar","title":"Statistical Properties and Applications of Word Tensors","speaker":"Mehrnoosh Sadrzadeh (Queen Mary University of London).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 01 June 2017","time":" 11:00 - 12:00","abstract":"Compositional distributional semantics represents meanings of phrases and sentences by vectors built from representations of the words therein. The categorical framework of Clark, Coecke, and myself offers a method whereby one builds these vectors by transforming the grammatical structure of the phrase/sentence into a linear map. In this framework, meanings of words with functional types become matrices, cubes, and in general higher order tensors. The work of Grefenstette, Kartsaklis, and I showed how different instantiations of the framework improve tasks such as phrase/sentence disambiguation,   similarity, and entailment. In recent work with Blundell and Jezek we show how these models are also helpful at the word level by employing them  in the verb similarity task of Gerz et al. With Kartsaklis and Ramgoolam, we propose that perturbed Gaussian models with permutation symmetry provide a framework for characterizing the statistical properties of the word tensors. In this talk, we will present the categorical framework and go through snippets of the experiments, with focus on the latter two recent advances.","bio":""},{"type":"seminar","title":"Location Resolution in Language Processing","speaker":"Milan Gritta (University of Cambridge).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 25 May 2017","time":" 11:00 - 12:00","abstract":"Detecting and monitoring disease outbreaks and epidemics using publicly available information sources (social media, blogs, news, forums, etc.) brings with it many technical challenges. In this talk, I will focus on the geographical aspect of information retrieval, outline the main hurdles and present a new computational method, which has been accepted to http://www.acl2017.org. I will also talk about some applications of this knowledge in future work. Some knowledge of computational linguistics is required to make the most of this talk.","bio":""},{"type":"seminar","title":"Reading and Reasoning with Vector Representations","speaker":"Sebastian Riedel (UCL).","affiliation":"","link":"","venue":"offline","place":"Lecture Block, room 2, Sidgwick Site","date":"Thursday 18 May 2017","time":" 11:00 - 12:00","abstract":"In recent years, vector representations of knowledge have become popular in  NLP  and beyond. They have at least two core benefits: reasoning with (low-dimensional) vectors tends to lead to better generalisation, and usually scales very well. But they raise their own set of questions: What type of inferences do they support? How can they capture asymmetry? How can explicit background knowledge be injected into vector-based architectures? How can we provide \u201cproofs\u201d that justify predictions? In this talk, I sketch some initial answers to some of these questions based on our recent work. In particular, I will illustrate how a vector space can simulate the workings of logic.","bio":"The world\'s largest seed biotech accelerator\\nHebrew Open Classes\\nCambridge Biomedical Research Centre \\"Distinguished Visitors\\" 2015 Lecture Series\\nNanoscience Centre Seminar Series\\nNanoDTC Talks\\nTalk about Jean Paul Sartre\\nOther talks\\nDouble talk on Autism genetics\\nSpeak white, speak black, speak American\\nFrontiers in paediatric cancer research\\nAdaptive auditory cortical coding of speech\\nFrom dry to wet granular media\\nPractical Steps to Addressing Unconscious / Implicit Bias \\n\\n\\n\\n\\n\xa0\\n\xa9 2006-2024 Talks.cam, University of Cambridge. Contact Us\\n | Help and Documentation\\n | Privacy and Publicity"},{"type":"seminar","title":"Building a True Semantic World: Generalizing set-theoretic semantics in vector spaces","speaker":"Eva Maria Vecchi.","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 11 May 2017","time":" 11:00 - 12:00","abstract":"Quantification is intrinsic to most utterances in natural language. It encompasses fundamental information which is a prerequisite for lexical semantics and inference tasks, such as hyponomy and entailment. Yet, we are unable to denote this information computationally in state-of-the-art semantic models. In this talk, I present an approach to automatically map a distributional semantic\\nspace onto a set-theoretic model to induce generalized quantifiers for subject-predicate pairs. I begin from the prediction that there is a functional relationship between distributional information and vectorial concept representations. I first introduce a large, computationally-friendly dataset motivated by understanding, theoretically, to which extent speakers agree on a single model of the world. Then I present a model to test this prediction and show that we can, indeed, map between formalisms, and further, we can generate natural language quantifiers sensibly.","bio":""},{"type":"seminar","title":"Cross-lingual Learning 2.0","speaker":"Anders S\xf8gaard (University of Copenhagen).","affiliation":"","link":"","venue":"offline","place":"GR06-7, English Faculty","date":"Thursday 27 April 2017","time":" 11:00 - 12:00","abstract":"I survey the last decade of work in cross-lingual transfer of  NLP  models. I briefly present the methods, but also the assumptions that have been made about the resources available to us when developing models for low-resource languages. I will discuss three trends: a) Cross-lingual learning is applied to more complex tasks, including frame semantics, discourse parsing, machine translation, and question answering. b) We make fewer and fewer assumptions. c) We have started exploring synergies between multiple transfer problems, potentially leading to \u201cuniversal\u201d models. Finally, I will discuss work in progress in our group to guide or regularize multi-task cross-lingual learning using auxiliary loss from readily available data.","bio":""},{"type":"seminar","title":"Semantic (Vector) Representations  of Word Senses, Concepts and Entities  and their Applications","speaker":"Jose Camacho-Collados (Sapienza University of Rome).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 20 April 2017","time":" 11:00 - 12:00","abstract":"A considerable amount of research has lately been conducted on developing neural architectures for learning vector representations of word forms (i.e., word embeddings). However, they have clear limitations when it comes to deep language understanding as they conflate different meanings of a word into a single representation and consequently are unable to accurately model semantics of individual word senses. A field of research has tried to address this issue with word representations by breaking them into those of their individual meanings. In this presentation I will give an overview of current representation techniques with a special emphasis on knowledge-based representations and  NASARI  (http://lcl.uniroma1.it/nasari/), our recently developed multilingual representation of concepts and entities. Finally, I will briefly present some of its most successful applications to date, namely semantic similarity, word and named entity disambiguation, sense clustering, domain labeling and text classification.","bio":"Jose Camacho Collados is a Google Doctoral Fellow and PhD student at Sapienza University of Rome (http://wwwusers.di.uniroma1.it/~collados). His research focuses on Natural Language Processing and on the area of lexical and distributional semantics in particular. Jose co-organized a tutorial on \u201cSemantic Representations of Word Senses and Concepts\u201d at  ACL 2016  (http://acl2016.org/index.php?article_id=58) and an  EACL 2017  workshop on \u201cSense, Concept and Entity Representations and their Applications\u201d (https://sites.google.com/site/senseworkshop2017/). He is additionally co-organizing a SemEval shared task on multilingual and cross-lingual semantic similarity (http://alt.qcri.org/semeval2017/task2/). His background education includes an Erasmus Mundus Master in Natural Language Processing and Human Language Technology and a 5-year BSc degree in Mathematics."},{"type":"seminar","title":"Scientific Paper Analysis: JST CREST Project of Big Data Application","speaker":"Prof. Yuji Matsumoto (Nara Institute of Science and Technology).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Tuesday 28 March 2017","time":" 11:00 - 12:00","abstract":"Rapid increase of scientific documents causes difficulty in acquiring up-to-date information even by experts.  This talk introduces an overview of the project of scientific document analysis supported by Japan Science and Technology Agency (JST).  In collaboration with the experts in Life Science, Material Science, Artificial Intelligence and other areas, we aim to develop an integrated environment for content-based document retrieval, summarization and knowledge extraction by analyzing the contents of large scale documents, and for survey generation.","bio":""},{"type":"seminar","title":"Knowledge Base Population from Text and Graphs","speaker":"Lucas Sterckx (University of  Ghent).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 09 March 2017","time":" 11:00 - 12:00","abstract":"Extracting knowledge from Web pages, and integrating it into a knowledge base (KB) is a task that spans the areas of natural language processing, information extraction, databases and machine learning. Recent years have witnessed a proliferation of large-scale KBs in academia and industry. Most prominently, major search engine providers (Google, Microsoft Bing, and Yahoo!) now include KBs in their products. This talk discusses mining and growing of knowledge bases from two perspectives.","bio":""},{"type":"seminar","title":"Computational approaches for deciphering the regulation of cancer genomes","speaker":"Shamith Samarajiwa.","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 23 February 2017","time":" 11:00 - 12:00","abstract":"Cancer consists of a collection of more than 200 different diseases and arise due to aberrant changes in the genome. Even within a single cancer type these aberrant mutations can be dissimilar and lead to tumour heterogeneity. Rapid technological advances in the last decade has given us the ability to \u201cread\u201d the genome sequence quickly and cheaply. Consequently, a number of large public cancer genome sequencing efforts have resulted in useful insights into carcinogenesis and treatment. The current state of cancer genomics will be briefly introduced.\\nMy lab utilises computational biology and data science approaches to understand carcinogenic processes and systems involved in cancer. This includes developing bespoke computational methods, tools and resources that are applied to different biomedical data types. We\u2019re particularly interested in using these different functional genomic data layers to understand their joint influence on rules underlying gene and epigenome regulation and how these are perturbed in cancer. Some of our data integration methods (including our foray into text mining and natural language processing) and resources used for building insightful maps and models of the cellular regulatory landscape will be discussed.","bio":""},{"type":"seminar","title":"Title to be confirmed","speaker":"Ekaterina Shutova.","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 16 February 2017","time":" 11:00 - 12:00","abstract":"Abstract not available","bio":""},{"type":"seminar","title":"Challenges and opportunities in the use of Internet data for insights on medicine","speaker":"Elad Yom-Tov (Microsoft Research).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 09 February 2017","time":" 11:00 - 12:00","abstract":"The majority of Internet users turn to the web for information when they have a medical concern. The data generated while users seek such information, and more generally when browse the Internet for work and pleasure, represent a potential boon for medical research. During the past decade these data have proven valuable where the most patient activity happens online, where internet data provides a more sensitive indicator than that attainable from traditional sources, and where reports from people suffer from significant reporting bias. However, extracting relevant insights entails overcoming several challenges, including processing of unstructured text, identification of relevant cohorts, causal inference from retrospective analysis, and the preservation of privacy.  \\nIn my talk I will exemplify some of the ways we have developed for overcoming the challenges in Internet data and the insights we have gained from these data. I will show how people who share the same medical condition can be identified from their anonymized search engine queries, making possible early screening for ovarian and cervical cancers as well as discovery of risk factors for disease. I will discuss the use of Internet data in evaluating the success of influenza vaccination campaigns, and for tracking the parameters of seasonal epidemics.","bio":""},{"type":"seminar","title":"Learning to Generate Textual Data","speaker":"Pontus Stenetorp (UCL).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 26 January 2017","time":" 11:00 - 12:00","abstract":"To learn text understanding models with millions of parameters one needs massive amounts of data.  In this work, we argue that generating data can compensate for this need.  While defining generic data generators is difficult, we propose to allow generators to be \u201cweakly\u201d specified in the sense that a set of parameters controls how the data is generated.  Consider for example generators where the example templates, grammar, and/or vocabulary is determined by this set of parameters.  Instead of manually tuning these parameters, we learn them from the limited training data at our disposal.  To achieve this, we derive an efficient algorithm called GeneRe that jointly estimates the parameters of the model and the undetermined generation parameters.  We illustrate its benefits by learning to solve math exam questions using a highly parametrised sequence-to-sequence neural network.","bio":""},{"type":"seminar","title":"Reflections on Universal Dependencies","speaker":"Joakim Nivre (Uppsala University).","affiliation":"","link":"","venue":"offline","place":"SR-24, English Faculty Building, 9 West Road (Sidgwick Site)","date":"Thursday 19 January 2017","time":" 11:00 - 12:00","abstract":"Universal Dependencies is a framework for cross-linguistically consistent treebank annotation that has so far been applied to over 50 languages. It was developed primarily to support multilingual parsing research, but the resources have proven useful for a wide range of studies that were not foreseen originally, including research on language typology. In this talk, I will give a general introduction to Universal Dependencies and the resources developed in the project, and survey a number of representative examples of how the resources\\nhave been put to use in different scientific investigations. I will conclude with some reflections on how Universal Dependencies relate to linguistic theory, on the one hand, and parsing technology, on the other.","bio":""},{"type":"seminar","title":"Sensing well-being using heterogeneous smartphone data and stance identification in social media conversations","speaker":"Maria Liakata (Warwick University).","affiliation":"","link":"","venue":"offline","place":"GR05, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 24 November 2016","time":" 11:00 - 12:00","abstract":"In the first part of my talk I will describe a new problem of predicting affect and well-being scales in a real-world setting of heterogeneous, longitudinal and non-synchronous textual as well as non-linguistic data that can be harvested from on-line media and mobile phones. We describe the method for collecting the heterogeneous longitudinal data, how features are extracted to address missing information and differences in temporal alignment, and how the latter are combined using multi-kernel learning to yield promising predictions of affect and well-being.\\nIn the second part of my talk I will discuss rumour stance classification as a sequential task. Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. We introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users\u2019 replies to one another, which results in a nested tree-like structure. Previous work addressing the stance classification task has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain  CRF  and Tree  CRF , each of which makes different assumptions about the conversational structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods.","bio":""},{"type":"seminar","title":"Industrial NLP Applications of Machine Learning in Different Domains","speaker":"Jochen Leidner (Thomson Reuters).","affiliation":"","link":"","venue":"offline","place":"GR05, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 17 November 2016","time":" 11:00 - 12:00","abstract":"The efficacy of applying machine learning to real-world problems has benefitted from the affordable availability of computational resources (CPU cycles,  RAM ) in abundance. In this presentation, I aim to present how computational linguistics and machine learning can be usefully combined to solve customer problems.\\n    To this end, I will describe some example applications of machine learning to problems from the financial, legal & regulatory and pharmacology domains, all using different use cases and expectations yet all based on natural language processing aided by supervised learning.\\n    I conclude by contrasting academic research from scientific research conducted in an industry environment.","bio":""},{"type":"seminar","title":"Monolingual and multilingual, explicit and latent vector representations of meaning","speaker":"Roberto Navigli (Sapienza University of Rome).","affiliation":"","link":"","venue":"offline","place":"GR05, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 10 November 2016","time":" 11:00 - 12:00","abstract":"In this talk I will present different kinds of representation of word senses and concepts. I will start with latent representations obtained as sense embeddings from the application of word2vec to the Wikipedia corpus, sense-tagged with a multilingual disambiguation algorithm based on BabelNet, the largest multilingual semantic network and encyclopedic dictionary covering 14 million concepts and entities and 271 languages.\\nI will then move on to two explicit vector representations of meaning (NASARI), based on lexical co-occurrence and multilingual semantic generalization, respectively, and a third latent version obtained from the word embeddings of the lexical vector.\\nExperimental results in several tasks, including word similarity, sense clustering, identification of sense predominance, and word sense disambiguation highlight high performance and show that, whenever a comparison is possible, sense representations consistently outperform word representations.\\nThis is joint work with Jos\xe9 Camacho-Collados, Ignacio Iacobacci and Mohammad Taher Pilehvar.","bio":""},{"type":"seminar","title":"Biomedical Text Mining: Structuring the Unstructured","speaker":"David Milward (Linguamatics).","affiliation":"","link":"","venue":"offline","place":"GR05, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 03 November 2016","time":" 11:00 - 12:00","abstract":"How can we capture information from biomedical text as conveniently as accessing a database? In this talk we will look at some of the challenges in processing diverse biomedical documents including scientific literature and electronic medical records. In addition to syntactic and semantic challenges, we will discuss the challenges of embedded tables and document structure.\\nMany of these challenges have been addressed over the last 15 years within the agile text mining platform,  I2E , allowing a wide range of practical applications in both the pharmaceutical industry and healthcare. The talk will conclude by showing how normalisation of concepts and relationships allows direct selection of patients using criteria such as \u201cbetween 18 and 65 years old, have cancer, weigh over 200lbs, and have a specific gene mutation\u201d.","bio":""},{"type":"seminar","title":"User-generated content mining: From collective disease rates to individual demographics","speaker":"Vasileios Lampos (UCL).","affiliation":"","link":"","venue":"offline","place":"GR05, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 27 October 2016","time":" 11:00 - 12:00","abstract":"Abstract not available","bio":""},{"type":"seminar","title":"Nursing Turing\'s Child: How to Grow Communication-based Intelligent Machines","speaker":"Marco Baroni (University of Trento/Facebook Artificial Intelligence Research).","affiliation":"","link":"","venue":"offline","place":"GR04, English Faculty, 9 West Road (Sidgwick Site)","date":"Monday 24 October 2016","time":" 15:30 - 17:00","abstract":"In this talk, I will outline a set of desiderata for general communication-based AI. I will then describe CommAI-env, a controlled environment we are currently developing, in which machines face a number of simple tasks designed to trigger the sort of basic abilities that artificial systems should possess in order to be able to interact with humans and learn further skills from them.\\nJoint work with Tomas Mikolov, Armand Joulin, Allan Jabri, Germ\xe1n Kruszewski, Angeliki Lazaridou and Klemen Simonic","bio":""},{"type":"seminar","title":"Building and using the Finnish Internet Parsebank","speaker":"Filip Ginter (University of Turku).","affiliation":"","link":"","venue":"offline","place":"GR05, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 20 October 2016","time":" 11:00 - 12:00","abstract":"The Finnish Internet Parsebank is a corpus of 270M Finnish sentences of Internet crawl data, syntactically analysed in the Universal Dependencies representation. I will present the parsebank, some of the lessons learned when crawling and analysing the data, the tools and derived resources we developed, and some of the uses the parsebank has seen. In particular, I  will focus on the syntax query tools which can efficiently handle a corpus of over 4 billion tokens of syntactically analysed data. I will also mention some future directions aiming at a similar parsebank for the majority of the languages in Universal Dependencies.","bio":""},{"type":"seminar","title":"Multiword Expressions and Compositionality Detection: Giving Word Embeddings a Hard Time","speaker":"Aline Villavicencio (Federal University of Rio Grande do Sul).","affiliation":"","link":"","venue":"offline","place":"GR04, English Faculty, 9 West Road (Sidgwick Site)","date":"Thursday 06 October 2016","time":" 11:00 - 12:00","abstract":"In this talk I start with an overview of Multiword Expressions (MWEs) like compound nouns and verb particle constructions, which have proved a challenge for computational analysis. These expressions need to be treated as a unit at some level of linguistic description. In particular, they display a wide range of compositionality, from more compositional cases like police car to more idiomatic MWEs like kick the bucket. Models for representing words and MWEs in semantic space, and their ability to capture compositionality/idiomaticity will be compared for three languages: English, French and Portuguese. The impact of some factors like the degree of corpus pre-processing and the size of context for the performance of these models will be discussed. I discuss the findings of a large-scale multilingual evaluation of DSMs for predicting the degree of semantic compositionality of nominal compounds on 4 datasets for English and French.","bio":""}]')}}]);
//# sourceMappingURL=517.fdb0ed02.chunk.js.map